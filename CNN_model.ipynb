{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "LpGSBCGusMJR",
        "outputId": "b903b924-4bdb-4cd0-9777-1f4bafcff7cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-203b55c9-9d7b-4ff9-b8eb-7027c8f3bf71\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-203b55c9-9d7b-4ff9-b8eb-7027c8f3bf71\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to upload your kaggle.json\n",
        "from google.colab import files\n",
        "files.upload() # Choose the kaggle.json file you downloaded\n",
        "\n",
        "# Make directory and move kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G-6TiWMsqoG",
        "outputId": "e45caba8-d339-448e-cf5d-98b8fa59504c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 87% 249M/285M [00:00<00:00, 565MB/s]\n",
            "100% 285M/285M [00:03<00:00, 94.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip -d fer2013_data"
      ],
      "metadata": {
        "id": "voY4fbEutGFL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/enelene/Facial-Expression-Recognition-Challenge.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAC7p-47tgsm",
        "outputId": "c764c2bf-3379-4bd4-8dcd-c65fe1750b67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Facial-Expression-Recognition-Challenge'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5m0UFPStkGP",
        "outputId": "73b5f7b8-2a3f-4e1c-ffc5-d8fd4adb58d2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Facial-Expression-Recognition-Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "PxPomYtpu0v9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "8QXvSFjmu2bZ",
        "outputId": "652ae26c-d52a-44ff-f19f-ddce007744b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megabe21\u001b[0m (\u001b[33megabe21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Vqqd7gS0u-Ea"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf ../fer2013_data/fer2013.tar.gz -C ../fer2013_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMT1pnpcxTLJ",
        "outputId": "94b5a8fc-ff55-40a1-fc41-4db344efdaa6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fer2013/fer2013.csv\n",
            "fer2013/README\n",
            "fer2013/fer2013.bib\n",
            "fer2013/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat ../fer2013_data/fer2013/README"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfig5ANnyEHm",
        "outputId": "56dc581f-5797-4e67-b32a-06529d59ad76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you use this dataset in your research work, please cite\n",
            "\n",
            "\"Challenges in Representation Learning: A report on three machine learning\n",
            "contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n",
            "Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,\n",
            "X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n",
            "M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n",
            "Y. Bengio. arXiv 2013.\n",
            "\n",
            "See fer2013.bib for a bibtex entry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Try loading train.csv first\n",
        "try:\n",
        "    train_df = pd.read_csv('fer2013_data/train.csv')\n",
        "    print(\"train.csv head:\")\n",
        "    print(train_df.head())\n",
        "    print(\"\\ntrain.csv columns:\", train_df.columns)\n",
        "    print(\"\\ntrain.csv info:\")\n",
        "    train_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"train.csv not found directly. Was fer2013.tar.gz extracted if it contained it?\")\n",
        "\n",
        "# Similarly for test.csv\n",
        "try:\n",
        "    test_df = pd.read_csv('fer2013_data/test.csv')\n",
        "    print(\"\\ntest.csv head:\")\n",
        "    print(test_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"test.csv not found directly.\")\n",
        "\n",
        "\n",
        "# And icml_face_data.csv\n",
        "try:\n",
        "    icml_df = pd.read_csv('fer2013_data/icml_face_data.csv')\n",
        "    print(\"\\nicml_face_data.csv head:\")\n",
        "    print(icml_df.head())\n",
        "    print(\"\\nicml_face_data.csv columns:\", icml_df.columns)\n",
        "    print(\"\\nicml_face_data.csv info:\")\n",
        "    icml_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"icml_face_data.csv not found directly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55gmEn8byPXN",
        "outputId": "15a2a2fa-5858-4ae4-a966-029b4ac97a2c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv head:\n",
            "   emotion                                             pixels\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "train.csv columns: Index(['emotion', 'pixels'], dtype='object')\n",
            "\n",
            "train.csv info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28709 entries, 0 to 28708\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  28709 non-null  int64 \n",
            " 1   pixels   28709 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 448.7+ KB\n",
            "\n",
            "test.csv head:\n",
            "                                              pixels\n",
            "0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...\n",
            "1  156 184 198 202 204 207 210 212 213 214 215 21...\n",
            "2  69 118 61 60 96 121 103 87 103 88 70 90 115 12...\n",
            "3  205 203 236 157 83 158 120 116 94 86 155 180 2...\n",
            "4  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...\n",
            "\n",
            "icml_face_data.csv head:\n",
            "   emotion     Usage                                             pixels\n",
            "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "icml_face_data.csv columns: Index(['emotion', ' Usage', ' pixels'], dtype='object')\n",
            "\n",
            "icml_face_data.csv info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1    Usage   35887 non-null  object\n",
            " 2    pixels  35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    fer2013_df = pd.read_csv('fer2013_data/fer2013/fer2013.csv')\n",
        "    print(\"\\nfer2013.csv head:\")\n",
        "    print(fer2013_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"test.csv not found directly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RucK_iqLyjKb",
        "outputId": "18ce199c-a298-4123-ad8a-cf8deb0e1d6d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fer2013.csv head:\n",
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'fer2013_data/icml_face_data.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "print(\"Cleaned column names:\", df.columns)\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "print(\"\\nEmotion distribution:\\n\", df['emotion'].value_counts())\n",
        "print(\"\\nUsage distribution:\\n\", df['Usage'].value_counts())\n",
        "\n",
        "# Emotion labels (as defined before)\n",
        "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "num_classes = len(emotion_labels)\n",
        "\n",
        "# --- 2. Split DataFrames based on 'Usage' column ---\n",
        "train_df = df[df['Usage'] == 'Training']\n",
        "val_df = df[df['Usage'] == 'PublicTest']\n",
        "test_df = df[df['Usage'] == 'PrivateTest']\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_df)}\")\n",
        "print(f\"Validation samples (PublicTest): {len(val_df)}\")\n",
        "print(f\"Test samples (PrivateTest): {len(test_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cA2LV-y0yF4",
        "outputId": "b0d0cf58-389e-4622-8fda-1751a51b406f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned column names: Index(['emotion', 'Usage', 'pixels'], dtype='object')\n",
            "   emotion     Usage                                             pixels\n",
            "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "Dataset shape: (35887, 3)\n",
            "\n",
            "Emotion distribution:\n",
            " emotion\n",
            "3    8989\n",
            "6    6198\n",
            "4    6077\n",
            "2    5121\n",
            "0    4953\n",
            "5    4002\n",
            "1     547\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Usage distribution:\n",
            " Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training samples: 28709\n",
            "Validation samples (PublicTest): 3589\n",
            "Test samples (PrivateTest): 3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.pixel_strings = self.df['pixels'].values\n",
        "        self.emotions = self.df['emotion'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert pixel string to numpy array and then to PIL Image\n",
        "        pixel_data = np.fromstring(self.pixel_strings[idx], dtype=int, sep=' ').reshape(48, 48).astype('uint8')\n",
        "        image = Image.fromarray(pixel_data) # Create PIL image from grayscale numpy array\n",
        "        emotion = torch.tensor(self.emotions[idx], dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, emotion"
      ],
      "metadata": {
        "id": "JHbURWfa0v4X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "if 'train_df' in locals():\n",
        "    train_dataset = FER2013Dataset(train_df, transform=train_transform)\n",
        "    val_dataset = FER2013Dataset(val_df, transform=val_test_transform) # Use PublicTest for validation\n",
        "    test_dataset = FER2013Dataset(test_df, transform=val_test_transform)  # Use PrivateTest for final testing\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Visualize a sample\n",
        "    def imshow(tensor_img, title=None):\n",
        "        img = tensor_img.clone().squeeze(0)\n",
        "        img = img * 0.5 + 0.5 # Unnormalize\n",
        "        plt.imshow(img.numpy(), cmap='gray')\n",
        "        if title is not None:\n",
        "            plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    try:\n",
        "        images, labels = next(iter(train_loader))\n",
        "        idx_to_show = 0\n",
        "        imshow(images[idx_to_show], title=f\"Label: {emotion_labels[labels[idx_to_show].item()]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not display image: {e}.\")\n",
        "else:\n",
        "    print(\"train_df not defined. Skipping DataLoader and visualization.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "S7WTTyBD0GuX",
        "outputId": "fb7d58de-b4fa-4c44-b2bd-05a1b663eef7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LKMN3bJk25xf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes_param):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Input is 1x48x48 (grayscale)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        # (16, 48, 48)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # (16, 24, 24)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        # (32, 24, 24)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # (32, 12, 12)\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        # 32 channels * 12 * 12 image size = 32 * 144 = 4608\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5) # Basic regularization\n",
        "        self.fc2 = nn.Linear(128, num_classes_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 12 * 12) # Flatten\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oU342k8I2uaA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'num_classes' not in globals():\n",
        "    print(\"Error: num_classes is not defined.\")\n",
        "    exit()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Ensure num_classes is defined before this line\n",
        "model = SimpleCNN(num_classes_param=num_classes).to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate_initial = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate_initial)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnrTBtQY2IiA",
        "outputId": "10c3aec5-8d48-4558-e7f9-3ecee412780f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "SimpleCNN(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_wandb = {\n",
        "    \"learning_rate\": learning_rate_initial,\n",
        "    \"architecture\": \"SimpleCNN_Baseline\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\",\n",
        "    \"epochs\": 15, # Start with a moderate number for the baseline\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.5,\n",
        "    \"notes\": \"Baseline model with 2 conv layers and 1 dropout layer in FC part.\"\n",
        "}\n",
        "\n",
        "# Start a new wandb run to track this script\n",
        "run = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb,\n",
        "    name=f\"run-{config_wandb['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\" # Unique run name\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cTwYmHmK3OAv",
        "outputId": "22540ccf-f036-452d-fe85-fc1a333007eb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Facial-Expression-Recognition-Challenge/wandb/run-20250602_185957-jnvyn0a6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">run-SimpleCNN_Baseline-20250602-185957</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_func(model_to_train, t_loader, v_loader, loss_criterion, opt, num_epochs_total, dev, current_wandb_run, run_name_prefix=\"train\"):\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    best_model_path = f\"{run_name_prefix}_best_model.pth\"\n",
        "\n",
        "    current_wandb_run.watch(model_to_train, loss_criterion, log=\"gradients\", log_freq=100) # log_freq is per batch\n",
        "\n",
        "    for epoch in range(num_epochs_total):\n",
        "        model_to_train.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(t_loader):\n",
        "            images = images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "\n",
        "            outputs = model_to_train(images)\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs_total}], Step [{i+1}/{len(t_loader)}], Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_train_loss = running_loss / len(t_loader.dataset)\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model_to_train.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in v_loader:\n",
        "                images = images.to(dev)\n",
        "                labels = labels.to(dev)\n",
        "                outputs = model_to_train(images)\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(v_loader.dataset)\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs_total}]:')\n",
        "        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}')\n",
        "        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
        "\n",
        "        # Log metrics to Wandb\n",
        "        current_wandb_run.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": epoch_train_loss,\n",
        "            \"train_accuracy\": epoch_train_acc,\n",
        "            \"val_loss\": epoch_val_loss,\n",
        "            \"val_accuracy\": epoch_val_acc,\n",
        "            \"learning_rate_current\": opt.param_groups[0]['lr'] # Log current LR\n",
        "        })\n",
        "\n",
        "        if epoch_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_acc\n",
        "            torch.save(model_to_train.state_dict(), best_model_path)\n",
        "            wandb.save(best_model_path) # This saves the file to Wandb's cloud storage for the run\n",
        "            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(f\"Best validation accuracy during training: {best_val_accuracy:.4f}\")\n",
        "    #loading the best model state for any final evaluations after the loop\n",
        "    model_to_train.load_state_dict(torch.load(best_model_path))\n",
        "    return best_model_path\n",
        "\n"
      ],
      "metadata": {
        "id": "ITGFrkNQAP9F"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the Training for the Baseline Model ---\n",
        "# Ensure train_loader and val_loader are defined and not empty\n",
        "if 'train_loader' in globals() and len(train_loader) > 0 and 'val_loader' in globals() and len(val_loader) > 0:\n",
        "    print(f\"Starting training for the baseline model: {config_wandb['architecture']}\")\n",
        "    best_model_file_baseline = train_model_func(model, train_loader, val_loader, criterion, optimizer,\n",
        "                                      num_epochs_total=config_wandb[\"epochs\"], dev=device,\n",
        "                                      current_wandb_run=run,\n",
        "                                      run_name_prefix=config_wandb[\"architecture\"])\n",
        "    print(f\"Best model from baseline training saved to: {best_model_file_baseline}\")\n",
        "\n",
        "    trained_model_artifact = wandb.Artifact(\n",
        "        name = f\"{config_wandb['architecture']}_final_model\",\n",
        "        type=\"model\",\n",
        "        description=f\"Final trained model state for {config_wandb['architecture']}\",\n",
        "        metadata=config_wandb # Add all hyperparameters\n",
        "    )\n",
        "    trained_model_artifact.add_file(best_model_file_baseline)\n",
        "    run.log_artifact(trained_model_artifact)\n",
        "    print(f\"Logged {best_model_file_baseline} as a Wandb artifact.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataLoaders (train_loader or val_loader) are not properly initialized or are empty. Skipping training.\")\n",
        "\n",
        "\n",
        "# --- 6. Finish the Wandb Run ---\n",
        "if 'run' in globals() and run is not None:\n",
        "    run.finish()\n",
        "    print(\"Wandb run finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSx9XWvgAmxw",
        "outputId": "ff89320e-6c02-4aaf-a24d-90df0fe1c6c4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for the baseline model: SimpleCNN_Baseline\n",
            "Epoch [1/15], Step [100/449], Batch Loss: 1.7912\n",
            "Epoch [1/15], Step [200/449], Batch Loss: 1.7170\n",
            "Epoch [1/15], Step [300/449], Batch Loss: 1.7050\n",
            "Epoch [1/15], Step [400/449], Batch Loss: 1.6390\n",
            "Epoch [1/15]:\n",
            "  Train Loss: 1.6875, Train Acc: 0.3251\n",
            "  Val Loss: 1.5243, Val Acc: 0.4171\n",
            "New best model saved with accuracy: 0.4171 at epoch 1\n",
            "Epoch [2/15], Step [100/449], Batch Loss: 1.5609\n",
            "Epoch [2/15], Step [200/449], Batch Loss: 1.4988\n",
            "Epoch [2/15], Step [300/449], Batch Loss: 1.5701\n",
            "Epoch [2/15], Step [400/449], Batch Loss: 1.6042\n",
            "Epoch [2/15]:\n",
            "  Train Loss: 1.5321, Train Acc: 0.4081\n",
            "  Val Loss: 1.4390, Val Acc: 0.4517\n",
            "New best model saved with accuracy: 0.4517 at epoch 2\n",
            "Epoch [3/15], Step [100/449], Batch Loss: 1.7228\n",
            "Epoch [3/15], Step [200/449], Batch Loss: 1.3938\n",
            "Epoch [3/15], Step [300/449], Batch Loss: 1.4110\n",
            "Epoch [3/15], Step [400/449], Batch Loss: 1.5964\n",
            "Epoch [3/15]:\n",
            "  Train Loss: 1.4583, Train Acc: 0.4390\n",
            "  Val Loss: 1.3785, Val Acc: 0.4759\n",
            "New best model saved with accuracy: 0.4759 at epoch 3\n",
            "Epoch [4/15], Step [100/449], Batch Loss: 1.3168\n",
            "Epoch [4/15], Step [200/449], Batch Loss: 1.4509\n",
            "Epoch [4/15], Step [300/449], Batch Loss: 1.1960\n",
            "Epoch [4/15], Step [400/449], Batch Loss: 1.3870\n",
            "Epoch [4/15]:\n",
            "  Train Loss: 1.4153, Train Acc: 0.4562\n",
            "  Val Loss: 1.3286, Val Acc: 0.4887\n",
            "New best model saved with accuracy: 0.4887 at epoch 4\n",
            "Epoch [5/15], Step [100/449], Batch Loss: 1.5642\n",
            "Epoch [5/15], Step [200/449], Batch Loss: 1.2544\n",
            "Epoch [5/15], Step [300/449], Batch Loss: 1.5011\n",
            "Epoch [5/15], Step [400/449], Batch Loss: 1.4554\n",
            "Epoch [5/15]:\n",
            "  Train Loss: 1.3787, Train Acc: 0.4733\n",
            "  Val Loss: 1.3095, Val Acc: 0.5124\n",
            "New best model saved with accuracy: 0.5124 at epoch 5\n",
            "Epoch [6/15], Step [100/449], Batch Loss: 1.3253\n",
            "Epoch [6/15], Step [200/449], Batch Loss: 1.3797\n",
            "Epoch [6/15], Step [300/449], Batch Loss: 1.4190\n",
            "Epoch [6/15], Step [400/449], Batch Loss: 1.3607\n",
            "Epoch [6/15]:\n",
            "  Train Loss: 1.3589, Train Acc: 0.4794\n",
            "  Val Loss: 1.2888, Val Acc: 0.5091\n",
            "Epoch [7/15], Step [100/449], Batch Loss: 1.3137\n",
            "Epoch [7/15], Step [200/449], Batch Loss: 1.4340\n",
            "Epoch [7/15], Step [300/449], Batch Loss: 1.2154\n",
            "Epoch [7/15], Step [400/449], Batch Loss: 1.2764\n",
            "Epoch [7/15]:\n",
            "  Train Loss: 1.3320, Train Acc: 0.4895\n",
            "  Val Loss: 1.2629, Val Acc: 0.5249\n",
            "New best model saved with accuracy: 0.5249 at epoch 7\n",
            "Epoch [8/15], Step [100/449], Batch Loss: 1.4654\n",
            "Epoch [8/15], Step [200/449], Batch Loss: 1.2493\n",
            "Epoch [8/15], Step [300/449], Batch Loss: 1.2640\n",
            "Epoch [8/15], Step [400/449], Batch Loss: 1.4209\n",
            "Epoch [8/15]:\n",
            "  Train Loss: 1.3091, Train Acc: 0.5002\n",
            "  Val Loss: 1.2506, Val Acc: 0.5213\n",
            "Epoch [9/15], Step [100/449], Batch Loss: 1.2129\n",
            "Epoch [9/15], Step [200/449], Batch Loss: 1.2317\n",
            "Epoch [9/15], Step [300/449], Batch Loss: 1.2773\n",
            "Epoch [9/15], Step [400/449], Batch Loss: 1.3161\n",
            "Epoch [9/15]:\n",
            "  Train Loss: 1.3014, Train Acc: 0.5040\n",
            "  Val Loss: 1.2394, Val Acc: 0.5241\n",
            "Epoch [10/15], Step [100/449], Batch Loss: 1.3810\n",
            "Epoch [10/15], Step [200/449], Batch Loss: 1.1801\n",
            "Epoch [10/15], Step [300/449], Batch Loss: 1.0886\n",
            "Epoch [10/15], Step [400/449], Batch Loss: 1.2775\n",
            "Epoch [10/15]:\n",
            "  Train Loss: 1.2827, Train Acc: 0.5076\n",
            "  Val Loss: 1.2232, Val Acc: 0.5336\n",
            "New best model saved with accuracy: 0.5336 at epoch 10\n",
            "Epoch [11/15], Step [100/449], Batch Loss: 1.1463\n",
            "Epoch [11/15], Step [200/449], Batch Loss: 1.3289\n",
            "Epoch [11/15], Step [300/449], Batch Loss: 1.3100\n",
            "Epoch [11/15], Step [400/449], Batch Loss: 1.3234\n",
            "Epoch [11/15]:\n",
            "  Train Loss: 1.2732, Train Acc: 0.5146\n",
            "  Val Loss: 1.2309, Val Acc: 0.5297\n",
            "Epoch [12/15], Step [100/449], Batch Loss: 1.3839\n",
            "Epoch [12/15], Step [200/449], Batch Loss: 1.2522\n",
            "Epoch [12/15], Step [300/449], Batch Loss: 1.4143\n",
            "Epoch [12/15], Step [400/449], Batch Loss: 1.2016\n",
            "Epoch [12/15]:\n",
            "  Train Loss: 1.2626, Train Acc: 0.5175\n",
            "  Val Loss: 1.2169, Val Acc: 0.5394\n",
            "New best model saved with accuracy: 0.5394 at epoch 12\n",
            "Epoch [13/15], Step [100/449], Batch Loss: 1.3256\n",
            "Epoch [13/15], Step [200/449], Batch Loss: 1.3017\n",
            "Epoch [13/15], Step [300/449], Batch Loss: 1.3149\n",
            "Epoch [13/15], Step [400/449], Batch Loss: 1.2374\n",
            "Epoch [13/15]:\n",
            "  Train Loss: 1.2488, Train Acc: 0.5234\n",
            "  Val Loss: 1.2091, Val Acc: 0.5375\n",
            "Epoch [14/15], Step [100/449], Batch Loss: 1.2010\n",
            "Epoch [14/15], Step [200/449], Batch Loss: 1.2222\n",
            "Epoch [14/15], Step [300/449], Batch Loss: 1.4022\n",
            "Epoch [14/15], Step [400/449], Batch Loss: 1.2487\n",
            "Epoch [14/15]:\n",
            "  Train Loss: 1.2382, Train Acc: 0.5280\n",
            "  Val Loss: 1.2001, Val Acc: 0.5425\n",
            "New best model saved with accuracy: 0.5425 at epoch 14\n",
            "Epoch [15/15], Step [100/449], Batch Loss: 1.1111\n",
            "Epoch [15/15], Step [200/449], Batch Loss: 1.3397\n",
            "Epoch [15/15], Step [300/449], Batch Loss: 1.3235\n",
            "Epoch [15/15], Step [400/449], Batch Loss: 1.3728\n",
            "Epoch [15/15]:\n",
            "  Train Loss: 1.2294, Train Acc: 0.5314\n",
            "  Val Loss: 1.2069, Val Acc: 0.5456\n",
            "New best model saved with accuracy: 0.5456 at epoch 15\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.5456\n",
            "Best model from baseline training saved to: SimpleCNN_Baseline_best_model.pth\n",
            "Logged SimpleCNN_Baseline_best_model.pth as a Wandb artifact.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>learning_rate_current</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.5314</td></tr><tr><td>train_loss</td><td>1.22942</td></tr><tr><td>val_accuracy</td><td>0.54556</td></tr><tr><td>val_loss</td><td>1.20688</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-SimpleCNN_Baseline-20250602-185957</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./Facial-Expression-Recognition-Challenge/wandb/run-20250602_185957-jnvyn0a6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run finished.\n"
          ]
        }
      ]
    }
  ]
}