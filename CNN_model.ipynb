{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxC1wDcbOtIDN5t6duAOda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enelene/Facial-Expression-Recognition-Challenge/blob/main/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "LpGSBCGusMJR",
        "outputId": "76262077-960f-4af8-da0c-699d1624169f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87ba9cce-c3f6-4e6c-987e-eb2a01ab82ab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-87ba9cce-c3f6-4e6c-987e-eb2a01ab82ab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to upload your kaggle.json\n",
        "from google.colab import files\n",
        "files.upload() # Choose the kaggle.json file you downloaded\n",
        "\n",
        "# Make directory and move kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G-6TiWMsqoG",
        "outputId": "4350eac9-f178-4b58-907d-e68459beb66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 89% 255M/285M [00:00<00:00, 833MB/s] \n",
            "100% 285M/285M [00:00<00:00, 874MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip -d fer2013_data"
      ],
      "metadata": {
        "id": "voY4fbEutGFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/enelene/Facial-Expression-Recognition-Challenge.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAC7p-47tgsm",
        "outputId": "6681a121-d9a9-496f-e3d0-38c5b8b3c4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Facial-Expression-Recognition-Challenge'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (7/7), 29.12 KiB | 29.12 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "PxPomYtpu0v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8QXvSFjmu2bZ",
        "outputId": "b7efce78-f478-4ac5-8855-80fd94d014df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megabe21\u001b[0m (\u001b[33megabe21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Vqqd7gS0u-Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf ../fer2013_data/fer2013.tar.gz -C ../fer2013_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMT1pnpcxTLJ",
        "outputId": "28409f97-1da6-4ccd-b018-71e4317a36cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): ../fer2013_data/fer2013.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat ../fer2013_data/fer2013/README"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfig5ANnyEHm",
        "outputId": "56dc581f-5797-4e67-b32a-06529d59ad76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you use this dataset in your research work, please cite\n",
            "\n",
            "\"Challenges in Representation Learning: A report on three machine learning\n",
            "contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n",
            "Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,\n",
            "X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n",
            "M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n",
            "Y. Bengio. arXiv 2013.\n",
            "\n",
            "See fer2013.bib for a bibtex entry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Try loading train.csv first\n",
        "try:\n",
        "    train_df = pd.read_csv('fer2013_data/train.csv')\n",
        "    print(\"train.csv head:\")\n",
        "    print(train_df.head())\n",
        "    print(\"\\ntrain.csv columns:\", train_df.columns)\n",
        "    print(\"\\ntrain.csv info:\")\n",
        "    train_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"train.csv not found directly. Was fer2013.tar.gz extracted if it contained it?\")\n",
        "\n",
        "# Similarly for test.csv\n",
        "try:\n",
        "    test_df = pd.read_csv('fer2013_data/test.csv')\n",
        "    print(\"\\ntest.csv head:\")\n",
        "    print(test_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"test.csv not found directly.\")\n",
        "\n",
        "\n",
        "# And icml_face_data.csv\n",
        "try:\n",
        "    icml_df = pd.read_csv('fer2013_data/icml_face_data.csv')\n",
        "    print(\"\\nicml_face_data.csv head:\")\n",
        "    print(icml_df.head())\n",
        "    print(\"\\nicml_face_data.csv columns:\", icml_df.columns)\n",
        "    print(\"\\nicml_face_data.csv info:\")\n",
        "    icml_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"icml_face_data.csv not found directly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55gmEn8byPXN",
        "outputId": "5a7fb5b0-ce0b-455c-be86-501042fe0bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv head:\n",
            "   emotion                                             pixels\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "train.csv columns: Index(['emotion', 'pixels'], dtype='object')\n",
            "\n",
            "train.csv info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28709 entries, 0 to 28708\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  28709 non-null  int64 \n",
            " 1   pixels   28709 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 448.7+ KB\n",
            "\n",
            "test.csv head:\n",
            "                                              pixels\n",
            "0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...\n",
            "1  156 184 198 202 204 207 210 212 213 214 215 21...\n",
            "2  69 118 61 60 96 121 103 87 103 88 70 90 115 12...\n",
            "3  205 203 236 157 83 158 120 116 94 86 155 180 2...\n",
            "4  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...\n",
            "\n",
            "icml_face_data.csv head:\n",
            "   emotion     Usage                                             pixels\n",
            "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "icml_face_data.csv columns: Index(['emotion', ' Usage', ' pixels'], dtype='object')\n",
            "\n",
            "icml_face_data.csv info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1    Usage   35887 non-null  object\n",
            " 2    pixels  35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    fer2013_df = pd.read_csv('fer2013_data/fer2013/fer2013.csv')\n",
        "    print(\"\\nfer2013.csv head:\")\n",
        "    print(fer2013_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"test.csv not found directly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RucK_iqLyjKb",
        "outputId": "6fd4039a-45fa-41dc-e653-24117e383405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.csv not found directly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'fer2013_data/icml_face_data.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "print(\"Cleaned column names:\", df.columns)\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "print(\"\\nEmotion distribution:\\n\", df['emotion'].value_counts())\n",
        "print(\"\\nUsage distribution:\\n\", df['Usage'].value_counts())\n",
        "\n",
        "# Emotion labels (as defined before)\n",
        "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "num_classes = len(emotion_labels)\n",
        "\n",
        "# --- 2. Split DataFrames based on 'Usage' column ---\n",
        "train_df = df[df['Usage'] == 'Training']\n",
        "val_df = df[df['Usage'] == 'PublicTest']\n",
        "test_df = df[df['Usage'] == 'PrivateTest']\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_df)}\")\n",
        "print(f\"Validation samples (PublicTest): {len(val_df)}\")\n",
        "print(f\"Test samples (PrivateTest): {len(test_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cA2LV-y0yF4",
        "outputId": "f87b2340-3f53-48a2-f239-80bd9eb8bdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned column names: Index(['emotion', 'Usage', 'pixels'], dtype='object')\n",
            "   emotion     Usage                                             pixels\n",
            "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "\n",
            "Dataset shape: (35887, 3)\n",
            "\n",
            "Emotion distribution:\n",
            " emotion\n",
            "3    8989\n",
            "6    6198\n",
            "4    6077\n",
            "2    5121\n",
            "0    4953\n",
            "5    4002\n",
            "1     547\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Usage distribution:\n",
            " Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training samples: 28709\n",
            "Validation samples (PublicTest): 3589\n",
            "Test samples (PrivateTest): 3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.pixel_strings = self.df['pixels'].values\n",
        "        self.emotions = self.df['emotion'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert pixel string to numpy array and then to PIL Image\n",
        "        pixel_data = np.fromstring(self.pixel_strings[idx], dtype=int, sep=' ').reshape(48, 48).astype('uint8')\n",
        "        image = Image.fromarray(pixel_data) # Create PIL image from grayscale numpy array\n",
        "        emotion = torch.tensor(self.emotions[idx], dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, emotion"
      ],
      "metadata": {
        "id": "JHbURWfa0v4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "if 'train_df' in locals():\n",
        "    train_dataset = FER2013Dataset(train_df, transform=train_transform)\n",
        "    val_dataset = FER2013Dataset(val_df, transform=val_test_transform) # Use PublicTest for validation\n",
        "    test_dataset = FER2013Dataset(test_df, transform=val_test_transform)  # Use PrivateTest for final testing\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Visualize a sample\n",
        "    def imshow(tensor_img, title=None):\n",
        "        img = tensor_img.clone().squeeze(0)\n",
        "        img = img * 0.5 + 0.5 # Unnormalize\n",
        "        plt.imshow(img.numpy(), cmap='gray')\n",
        "        if title is not None:\n",
        "            plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    try:\n",
        "        images, labels = next(iter(train_loader))\n",
        "        idx_to_show = 0\n",
        "        imshow(images[idx_to_show], title=f\"Label: {emotion_labels[labels[idx_to_show].item()]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not display image: {e}.\")\n",
        "else:\n",
        "    print(\"train_df not defined. Skipping DataLoader and visualization.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "S7WTTyBD0GuX",
        "outputId": "95ed60b1-0f4a-4f58-eb70-386b91aed94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LKMN3bJk25xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes_param):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Input is 1x48x48 (grayscale)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        # (16, 48, 48)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # (16, 24, 24)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        # (32, 24, 24)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # (32, 12, 12)\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        # 32 channels * 12 * 12 image size = 32 * 144 = 4608\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5) # Basic regularization\n",
        "        self.fc2 = nn.Linear(128, num_classes_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 12 * 12) # Flatten\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oU342k8I2uaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'num_classes' not in globals():\n",
        "    print(\"Error: num_classes is not defined.\")\n",
        "    exit()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Ensure num_classes is defined before this line\n",
        "model = SimpleCNN(num_classes_param=num_classes).to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate_initial = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate_initial)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnrTBtQY2IiA",
        "outputId": "39e2ffa7-ec6d-4ef4-92f9-21d1b2a86b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "SimpleCNN(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_wandb = {\n",
        "    \"learning_rate\": learning_rate_initial,\n",
        "    \"architecture\": \"SimpleCNN_Baseline\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\",\n",
        "    \"epochs\": 15, # Start with a moderate number for the baseline\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.5,\n",
        "    \"notes\": \"Baseline model with 2 conv layers and 1 dropout layer in FC part.\"\n",
        "}\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb,\n",
        "    name=f\"run-{config_wandb['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cTwYmHmK3OAv",
        "outputId": "22540ccf-f036-452d-fe85-fc1a333007eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Facial-Expression-Recognition-Challenge/wandb/run-20250602_185957-jnvyn0a6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">run-SimpleCNN_Baseline-20250602-185957</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_func(model_to_train, t_loader, v_loader, loss_criterion, opt, num_epochs_total, dev, current_wandb_run, run_name_prefix=\"train\"):\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    best_model_path = f\"{run_name_prefix}_best_model.pth\"\n",
        "\n",
        "    current_wandb_run.watch(model_to_train, loss_criterion, log=\"gradients\", log_freq=100)\n",
        "\n",
        "    for epoch in range(num_epochs_total):\n",
        "        model_to_train.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(t_loader):\n",
        "            images = images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "\n",
        "            outputs = model_to_train(images)\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs_total}], Step [{i+1}/{len(t_loader)}], Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_train_loss = running_loss / len(t_loader.dataset)\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model_to_train.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in v_loader:\n",
        "                images = images.to(dev)\n",
        "                labels = labels.to(dev)\n",
        "                outputs = model_to_train(images)\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(v_loader.dataset)\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs_total}]:')\n",
        "        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}')\n",
        "        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
        "\n",
        "        # Log metrics to Wandb\n",
        "        current_wandb_run.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": epoch_train_loss,\n",
        "            \"train_accuracy\": epoch_train_acc,\n",
        "            \"val_loss\": epoch_val_loss,\n",
        "            \"val_accuracy\": epoch_val_acc,\n",
        "            \"learning_rate_current\": opt.param_groups[0]['lr'] # Log current LR\n",
        "        })\n",
        "\n",
        "        if epoch_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_acc\n",
        "            torch.save(model_to_train.state_dict(), best_model_path)\n",
        "            wandb.save(best_model_path) # This saves the file to Wandb's cloud storage for the run\n",
        "            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f} at epoch {epoch+1}\")\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(f\"Best validation accuracy during training: {best_val_accuracy:.4f}\")\n",
        "    #loading the best model state for any final evaluations after the loop\n",
        "    model_to_train.load_state_dict(torch.load(best_model_path))\n",
        "    return best_model_path\n",
        "\n"
      ],
      "metadata": {
        "id": "ITGFrkNQAP9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'train_loader' in globals() and len(train_loader) > 0 and 'val_loader' in globals() and len(val_loader) > 0:\n",
        "    print(f\"Starting training for the baseline model: {config_wandb['architecture']}\")\n",
        "    best_model_file_baseline = train_model_func(model, train_loader, val_loader, criterion, optimizer,\n",
        "                                      num_epochs_total=config_wandb[\"epochs\"], dev=device,\n",
        "                                      current_wandb_run=run,\n",
        "                                      run_name_prefix=config_wandb[\"architecture\"])\n",
        "    print(f\"Best model from baseline training saved to: {best_model_file_baseline}\")\n",
        "\n",
        "    trained_model_artifact = wandb.Artifact(\n",
        "        name = f\"{config_wandb['architecture']}_final_model\",\n",
        "        type=\"model\",\n",
        "        description=f\"Final trained model state for {config_wandb['architecture']}\",\n",
        "        metadata=config_wandb # Add all hyperparameters\n",
        "    )\n",
        "    trained_model_artifact.add_file(best_model_file_baseline)\n",
        "    run.log_artifact(trained_model_artifact)\n",
        "    print(f\"Logged {best_model_file_baseline} as a Wandb artifact.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataLoaders (train_loader or val_loader) are not properly initialized or are empty. Skipping training.\")\n",
        "\n",
        "if 'run' in globals() and run is not None:\n",
        "    run.finish()\n",
        "    print(\"Wandb run finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSx9XWvgAmxw",
        "outputId": "ff89320e-6c02-4aaf-a24d-90df0fe1c6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for the baseline model: SimpleCNN_Baseline\n",
            "Epoch [1/15], Step [100/449], Batch Loss: 1.7912\n",
            "Epoch [1/15], Step [200/449], Batch Loss: 1.7170\n",
            "Epoch [1/15], Step [300/449], Batch Loss: 1.7050\n",
            "Epoch [1/15], Step [400/449], Batch Loss: 1.6390\n",
            "Epoch [1/15]:\n",
            "  Train Loss: 1.6875, Train Acc: 0.3251\n",
            "  Val Loss: 1.5243, Val Acc: 0.4171\n",
            "New best model saved with accuracy: 0.4171 at epoch 1\n",
            "Epoch [2/15], Step [100/449], Batch Loss: 1.5609\n",
            "Epoch [2/15], Step [200/449], Batch Loss: 1.4988\n",
            "Epoch [2/15], Step [300/449], Batch Loss: 1.5701\n",
            "Epoch [2/15], Step [400/449], Batch Loss: 1.6042\n",
            "Epoch [2/15]:\n",
            "  Train Loss: 1.5321, Train Acc: 0.4081\n",
            "  Val Loss: 1.4390, Val Acc: 0.4517\n",
            "New best model saved with accuracy: 0.4517 at epoch 2\n",
            "Epoch [3/15], Step [100/449], Batch Loss: 1.7228\n",
            "Epoch [3/15], Step [200/449], Batch Loss: 1.3938\n",
            "Epoch [3/15], Step [300/449], Batch Loss: 1.4110\n",
            "Epoch [3/15], Step [400/449], Batch Loss: 1.5964\n",
            "Epoch [3/15]:\n",
            "  Train Loss: 1.4583, Train Acc: 0.4390\n",
            "  Val Loss: 1.3785, Val Acc: 0.4759\n",
            "New best model saved with accuracy: 0.4759 at epoch 3\n",
            "Epoch [4/15], Step [100/449], Batch Loss: 1.3168\n",
            "Epoch [4/15], Step [200/449], Batch Loss: 1.4509\n",
            "Epoch [4/15], Step [300/449], Batch Loss: 1.1960\n",
            "Epoch [4/15], Step [400/449], Batch Loss: 1.3870\n",
            "Epoch [4/15]:\n",
            "  Train Loss: 1.4153, Train Acc: 0.4562\n",
            "  Val Loss: 1.3286, Val Acc: 0.4887\n",
            "New best model saved with accuracy: 0.4887 at epoch 4\n",
            "Epoch [5/15], Step [100/449], Batch Loss: 1.5642\n",
            "Epoch [5/15], Step [200/449], Batch Loss: 1.2544\n",
            "Epoch [5/15], Step [300/449], Batch Loss: 1.5011\n",
            "Epoch [5/15], Step [400/449], Batch Loss: 1.4554\n",
            "Epoch [5/15]:\n",
            "  Train Loss: 1.3787, Train Acc: 0.4733\n",
            "  Val Loss: 1.3095, Val Acc: 0.5124\n",
            "New best model saved with accuracy: 0.5124 at epoch 5\n",
            "Epoch [6/15], Step [100/449], Batch Loss: 1.3253\n",
            "Epoch [6/15], Step [200/449], Batch Loss: 1.3797\n",
            "Epoch [6/15], Step [300/449], Batch Loss: 1.4190\n",
            "Epoch [6/15], Step [400/449], Batch Loss: 1.3607\n",
            "Epoch [6/15]:\n",
            "  Train Loss: 1.3589, Train Acc: 0.4794\n",
            "  Val Loss: 1.2888, Val Acc: 0.5091\n",
            "Epoch [7/15], Step [100/449], Batch Loss: 1.3137\n",
            "Epoch [7/15], Step [200/449], Batch Loss: 1.4340\n",
            "Epoch [7/15], Step [300/449], Batch Loss: 1.2154\n",
            "Epoch [7/15], Step [400/449], Batch Loss: 1.2764\n",
            "Epoch [7/15]:\n",
            "  Train Loss: 1.3320, Train Acc: 0.4895\n",
            "  Val Loss: 1.2629, Val Acc: 0.5249\n",
            "New best model saved with accuracy: 0.5249 at epoch 7\n",
            "Epoch [8/15], Step [100/449], Batch Loss: 1.4654\n",
            "Epoch [8/15], Step [200/449], Batch Loss: 1.2493\n",
            "Epoch [8/15], Step [300/449], Batch Loss: 1.2640\n",
            "Epoch [8/15], Step [400/449], Batch Loss: 1.4209\n",
            "Epoch [8/15]:\n",
            "  Train Loss: 1.3091, Train Acc: 0.5002\n",
            "  Val Loss: 1.2506, Val Acc: 0.5213\n",
            "Epoch [9/15], Step [100/449], Batch Loss: 1.2129\n",
            "Epoch [9/15], Step [200/449], Batch Loss: 1.2317\n",
            "Epoch [9/15], Step [300/449], Batch Loss: 1.2773\n",
            "Epoch [9/15], Step [400/449], Batch Loss: 1.3161\n",
            "Epoch [9/15]:\n",
            "  Train Loss: 1.3014, Train Acc: 0.5040\n",
            "  Val Loss: 1.2394, Val Acc: 0.5241\n",
            "Epoch [10/15], Step [100/449], Batch Loss: 1.3810\n",
            "Epoch [10/15], Step [200/449], Batch Loss: 1.1801\n",
            "Epoch [10/15], Step [300/449], Batch Loss: 1.0886\n",
            "Epoch [10/15], Step [400/449], Batch Loss: 1.2775\n",
            "Epoch [10/15]:\n",
            "  Train Loss: 1.2827, Train Acc: 0.5076\n",
            "  Val Loss: 1.2232, Val Acc: 0.5336\n",
            "New best model saved with accuracy: 0.5336 at epoch 10\n",
            "Epoch [11/15], Step [100/449], Batch Loss: 1.1463\n",
            "Epoch [11/15], Step [200/449], Batch Loss: 1.3289\n",
            "Epoch [11/15], Step [300/449], Batch Loss: 1.3100\n",
            "Epoch [11/15], Step [400/449], Batch Loss: 1.3234\n",
            "Epoch [11/15]:\n",
            "  Train Loss: 1.2732, Train Acc: 0.5146\n",
            "  Val Loss: 1.2309, Val Acc: 0.5297\n",
            "Epoch [12/15], Step [100/449], Batch Loss: 1.3839\n",
            "Epoch [12/15], Step [200/449], Batch Loss: 1.2522\n",
            "Epoch [12/15], Step [300/449], Batch Loss: 1.4143\n",
            "Epoch [12/15], Step [400/449], Batch Loss: 1.2016\n",
            "Epoch [12/15]:\n",
            "  Train Loss: 1.2626, Train Acc: 0.5175\n",
            "  Val Loss: 1.2169, Val Acc: 0.5394\n",
            "New best model saved with accuracy: 0.5394 at epoch 12\n",
            "Epoch [13/15], Step [100/449], Batch Loss: 1.3256\n",
            "Epoch [13/15], Step [200/449], Batch Loss: 1.3017\n",
            "Epoch [13/15], Step [300/449], Batch Loss: 1.3149\n",
            "Epoch [13/15], Step [400/449], Batch Loss: 1.2374\n",
            "Epoch [13/15]:\n",
            "  Train Loss: 1.2488, Train Acc: 0.5234\n",
            "  Val Loss: 1.2091, Val Acc: 0.5375\n",
            "Epoch [14/15], Step [100/449], Batch Loss: 1.2010\n",
            "Epoch [14/15], Step [200/449], Batch Loss: 1.2222\n",
            "Epoch [14/15], Step [300/449], Batch Loss: 1.4022\n",
            "Epoch [14/15], Step [400/449], Batch Loss: 1.2487\n",
            "Epoch [14/15]:\n",
            "  Train Loss: 1.2382, Train Acc: 0.5280\n",
            "  Val Loss: 1.2001, Val Acc: 0.5425\n",
            "New best model saved with accuracy: 0.5425 at epoch 14\n",
            "Epoch [15/15], Step [100/449], Batch Loss: 1.1111\n",
            "Epoch [15/15], Step [200/449], Batch Loss: 1.3397\n",
            "Epoch [15/15], Step [300/449], Batch Loss: 1.3235\n",
            "Epoch [15/15], Step [400/449], Batch Loss: 1.3728\n",
            "Epoch [15/15]:\n",
            "  Train Loss: 1.2294, Train Acc: 0.5314\n",
            "  Val Loss: 1.2069, Val Acc: 0.5456\n",
            "New best model saved with accuracy: 0.5456 at epoch 15\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.5456\n",
            "Best model from baseline training saved to: SimpleCNN_Baseline_best_model.pth\n",
            "Logged SimpleCNN_Baseline_best_model.pth as a Wandb artifact.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>learning_rate_current</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.5314</td></tr><tr><td>train_loss</td><td>1.22942</td></tr><tr><td>val_accuracy</td><td>0.54556</td></tr><tr><td>val_loss</td><td>1.20688</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-SimpleCNN_Baseline-20250602-185957</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/jnvyn0a6</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./Facial-Expression-Recognition-Challenge/wandb/run-20250602_185957-jnvyn0a6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: DeeperCNN"
      ],
      "metadata": {
        "id": "RUxgGToMFxo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self, num_classes_param):\n",
        "        super(DeeperCNN, self).__init__() # Call the constructor of the parent nn.Module class\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        # Input: (Batch_Size, 1, 48, 48)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        # Output: (Batch_Size, 32, 48, 48)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Output: (Batch_Size, 32, 24, 24)\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        # Output: (Batch_Size, 64, 24, 24)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Output: (Batch_Size, 64, 12, 12)\n",
        "\n",
        "        # Convolutional Block 3 (NEW)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        # Output: (Batch_Size, 128, 12, 12)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Output: (Batch_Size, 128, 6, 6)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=128 * 6 * 6, out_features=256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5) # Same dropout rate as baseline for now\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=num_classes_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Block 3 (NEW)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = x.view(-1, 128 * 6 * 6)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CTVHNokOFy70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deeper_model = DeeperCNN(num_classes_param=num_classes).to(device)\n",
        "print(\"--- DeeperCNN Architecture ---\")\n",
        "print(deeper_model)\n",
        "\n",
        "learning_rate_exp2 = 0.001\n",
        "optimizer_exp2 = optim.Adam(deeper_model.parameters(), lr=learning_rate_exp2)\n",
        "\n",
        "config_wandb_exp2 = {\n",
        "    \"learning_rate\": learning_rate_exp2,\n",
        "    \"architecture\": \"DeeperCNN_v1\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\", # Same dataset\n",
        "    \"epochs\": 15,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.5,\n",
        "    \"conv1_filters\": 32,\n",
        "    \"conv2_filters\": 64,\n",
        "    \"conv3_filters\": 128,\n",
        "    \"fc1_units\": 256,\n",
        "    \"notes\": \"Experiment 2: Deeper CNN with 3 conv blocks (32,64,128 filters) and 256 FC units.\"\n",
        "}\n",
        "\n",
        "run_exp2 = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb_exp2,\n",
        "    name=f\"run-{config_wandb_exp2['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    reinit=True\n",
        ")\n",
        "print(f\"Wandb run for {config_wandb_exp2['architecture']} initialized: {run_exp2.url}\")\n",
        "\n",
        "if 'train_loader' in globals() and 'val_loader' in globals() and 'criterion' in globals():\n",
        "    print(f\"\\nStarting training for: {config_wandb_exp2['architecture']}\")\n",
        "\n",
        "    best_model_file_exp2 = train_model_func(\n",
        "        model_to_train=deeper_model,\n",
        "        t_loader=train_loader,\n",
        "        v_loader=val_loader,\n",
        "        loss_criterion=criterion,\n",
        "        opt=optimizer_exp2,\n",
        "        num_epochs_total=config_wandb_exp2[\"epochs\"],\n",
        "        dev=device,\n",
        "        current_wandb_run=run_exp2,\n",
        "        run_name_prefix=config_wandb_exp2[\"architecture\"]\n",
        "    )\n",
        "    print(f\"Best model from {config_wandb_exp2['architecture']} training saved to: {best_model_file_exp2}\")\n",
        "\n",
        "    trained_model_artifact_exp2 = wandb.Artifact(\n",
        "        name=f\"{config_wandb_exp2['architecture']}_model\", # Unique artifact name\n",
        "        type=\"model\",\n",
        "        description=f\"Trained model state for {config_wandb_exp2['architecture']}\",\n",
        "        metadata=config_wandb_exp2\n",
        "    )\n",
        "    trained_model_artifact_exp2.add_file(best_model_file_exp2)\n",
        "    run_exp2.log_artifact(trained_model_artifact_exp2)\n",
        "    print(f\"Logged {best_model_file_exp2} as a Wandb artifact for run {run_exp2.name}.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: DataLoaders (train_loader, val_loader) or criterion not found. Please ensure they are defined.\")\n",
        "\n",
        "if 'run_exp2' in globals() and run_exp2 is not None:\n",
        "    run_exp2.finish()\n",
        "    print(f\"Wandb run {run_exp2.name} finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A-NB6iQ4GUDA",
        "outputId": "c0233aac-25cd-4f72-fb2d-ba69f3c07df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DeeperCNN Architecture ---\n",
            "DeeperCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu3): ReLU()\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=256, bias=True)\n",
            "  (relu4): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_141737-cht7k3iq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq' target=\"_blank\">run-DeeperCNN_v1-20250605-141737</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run for DeeperCNN_v1 initialized: https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq\n",
            "\n",
            "Starting training for: DeeperCNN_v1\n",
            "Epoch [1/15], Step [100/449], Batch Loss: 1.7830\n",
            "Epoch [1/15], Step [200/449], Batch Loss: 1.6605\n",
            "Epoch [1/15], Step [300/449], Batch Loss: 1.6541\n",
            "Epoch [1/15], Step [400/449], Batch Loss: 1.5594\n",
            "Epoch [1/15]:\n",
            "  Train Loss: 1.6832, Train Acc: 0.3227\n",
            "  Val Loss: 1.5104, Val Acc: 0.4202\n",
            "New best model saved with accuracy: 0.4202 at epoch 1\n",
            "Epoch [2/15], Step [100/449], Batch Loss: 1.4990\n",
            "Epoch [2/15], Step [200/449], Batch Loss: 1.4863\n",
            "Epoch [2/15], Step [300/449], Batch Loss: 1.1759\n",
            "Epoch [2/15], Step [400/449], Batch Loss: 1.2924\n",
            "Epoch [2/15]:\n",
            "  Train Loss: 1.4594, Train Acc: 0.4391\n",
            "  Val Loss: 1.3624, Val Acc: 0.4834\n",
            "New best model saved with accuracy: 0.4834 at epoch 2\n",
            "Epoch [3/15], Step [100/449], Batch Loss: 1.4039\n",
            "Epoch [3/15], Step [200/449], Batch Loss: 1.3212\n",
            "Epoch [3/15], Step [300/449], Batch Loss: 1.2194\n",
            "Epoch [3/15], Step [400/449], Batch Loss: 1.3631\n",
            "Epoch [3/15]:\n",
            "  Train Loss: 1.3442, Train Acc: 0.4882\n",
            "  Val Loss: 1.2631, Val Acc: 0.5107\n",
            "New best model saved with accuracy: 0.5107 at epoch 3\n",
            "Epoch [4/15], Step [100/449], Batch Loss: 1.3527\n",
            "Epoch [4/15], Step [200/449], Batch Loss: 1.2621\n",
            "Epoch [4/15], Step [300/449], Batch Loss: 1.2822\n",
            "Epoch [4/15], Step [400/449], Batch Loss: 1.5007\n",
            "Epoch [4/15]:\n",
            "  Train Loss: 1.2785, Train Acc: 0.5127\n",
            "  Val Loss: 1.2535, Val Acc: 0.5252\n",
            "New best model saved with accuracy: 0.5252 at epoch 4\n",
            "Epoch [5/15], Step [100/449], Batch Loss: 1.4401\n",
            "Epoch [5/15], Step [200/449], Batch Loss: 1.0705\n",
            "Epoch [5/15], Step [300/449], Batch Loss: 1.0788\n",
            "Epoch [5/15], Step [400/449], Batch Loss: 1.3098\n",
            "Epoch [5/15]:\n",
            "  Train Loss: 1.2335, Train Acc: 0.5306\n",
            "  Val Loss: 1.1916, Val Acc: 0.5425\n",
            "New best model saved with accuracy: 0.5425 at epoch 5\n",
            "Epoch [6/15], Step [100/449], Batch Loss: 1.2044\n",
            "Epoch [6/15], Step [200/449], Batch Loss: 1.0477\n",
            "Epoch [6/15], Step [300/449], Batch Loss: 1.2385\n",
            "Epoch [6/15], Step [400/449], Batch Loss: 1.2838\n",
            "Epoch [6/15]:\n",
            "  Train Loss: 1.2000, Train Acc: 0.5468\n",
            "  Val Loss: 1.1508, Val Acc: 0.5478\n",
            "New best model saved with accuracy: 0.5478 at epoch 6\n",
            "Epoch [7/15], Step [100/449], Batch Loss: 1.1676\n",
            "Epoch [7/15], Step [200/449], Batch Loss: 1.0619\n",
            "Epoch [7/15], Step [300/449], Batch Loss: 1.1892\n",
            "Epoch [7/15], Step [400/449], Batch Loss: 1.1460\n",
            "Epoch [7/15]:\n",
            "  Train Loss: 1.1659, Train Acc: 0.5574\n",
            "  Val Loss: 1.1617, Val Acc: 0.5559\n",
            "New best model saved with accuracy: 0.5559 at epoch 7\n",
            "Epoch [8/15], Step [100/449], Batch Loss: 0.9721\n",
            "Epoch [8/15], Step [200/449], Batch Loss: 0.8768\n",
            "Epoch [8/15], Step [300/449], Batch Loss: 1.2880\n",
            "Epoch [8/15], Step [400/449], Batch Loss: 1.1087\n",
            "Epoch [8/15]:\n",
            "  Train Loss: 1.1454, Train Acc: 0.5685\n",
            "  Val Loss: 1.1470, Val Acc: 0.5609\n",
            "New best model saved with accuracy: 0.5609 at epoch 8\n",
            "Epoch [9/15], Step [100/449], Batch Loss: 1.0348\n",
            "Epoch [9/15], Step [200/449], Batch Loss: 1.0378\n",
            "Epoch [9/15], Step [300/449], Batch Loss: 1.2471\n",
            "Epoch [9/15], Step [400/449], Batch Loss: 1.1232\n",
            "Epoch [9/15]:\n",
            "  Train Loss: 1.1283, Train Acc: 0.5756\n",
            "  Val Loss: 1.1297, Val Acc: 0.5715\n",
            "New best model saved with accuracy: 0.5715 at epoch 9\n",
            "Epoch [10/15], Step [100/449], Batch Loss: 1.1106\n",
            "Epoch [10/15], Step [200/449], Batch Loss: 1.2799\n",
            "Epoch [10/15], Step [300/449], Batch Loss: 1.0297\n",
            "Epoch [10/15], Step [400/449], Batch Loss: 1.1261\n",
            "Epoch [10/15]:\n",
            "  Train Loss: 1.1062, Train Acc: 0.5813\n",
            "  Val Loss: 1.1095, Val Acc: 0.5748\n",
            "New best model saved with accuracy: 0.5748 at epoch 10\n",
            "Epoch [11/15], Step [100/449], Batch Loss: 1.1385\n",
            "Epoch [11/15], Step [200/449], Batch Loss: 1.2563\n",
            "Epoch [11/15], Step [300/449], Batch Loss: 1.0066\n",
            "Epoch [11/15], Step [400/449], Batch Loss: 1.0766\n",
            "Epoch [11/15]:\n",
            "  Train Loss: 1.0876, Train Acc: 0.5931\n",
            "  Val Loss: 1.1127, Val Acc: 0.5726\n",
            "Epoch [12/15], Step [100/449], Batch Loss: 1.0976\n",
            "Epoch [12/15], Step [200/449], Batch Loss: 0.8743\n",
            "Epoch [12/15], Step [300/449], Batch Loss: 1.0339\n",
            "Epoch [12/15], Step [400/449], Batch Loss: 1.2331\n",
            "Epoch [12/15]:\n",
            "  Train Loss: 1.0741, Train Acc: 0.5970\n",
            "  Val Loss: 1.0967, Val Acc: 0.5815\n",
            "New best model saved with accuracy: 0.5815 at epoch 12\n",
            "Epoch [13/15], Step [100/449], Batch Loss: 1.0542\n",
            "Epoch [13/15], Step [200/449], Batch Loss: 0.8314\n",
            "Epoch [13/15], Step [300/449], Batch Loss: 0.8613\n",
            "Epoch [13/15], Step [400/449], Batch Loss: 1.1151\n",
            "Epoch [13/15]:\n",
            "  Train Loss: 1.0619, Train Acc: 0.5988\n",
            "  Val Loss: 1.1079, Val Acc: 0.5787\n",
            "Epoch [14/15], Step [100/449], Batch Loss: 1.0392\n",
            "Epoch [14/15], Step [200/449], Batch Loss: 1.0848\n",
            "Epoch [14/15], Step [300/449], Batch Loss: 1.0104\n",
            "Epoch [14/15], Step [400/449], Batch Loss: 1.1015\n",
            "Epoch [14/15]:\n",
            "  Train Loss: 1.0478, Train Acc: 0.6076\n",
            "  Val Loss: 1.1149, Val Acc: 0.5795\n",
            "Epoch [15/15], Step [100/449], Batch Loss: 1.1406\n",
            "Epoch [15/15], Step [200/449], Batch Loss: 1.0613\n",
            "Epoch [15/15], Step [300/449], Batch Loss: 0.8721\n",
            "Epoch [15/15], Step [400/449], Batch Loss: 1.0912\n",
            "Epoch [15/15]:\n",
            "  Train Loss: 1.0350, Train Acc: 0.6126\n",
            "  Val Loss: 1.0959, Val Acc: 0.5837\n",
            "New best model saved with accuracy: 0.5837 at epoch 15\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.5837\n",
            "Best model from DeeperCNN_v1 training saved to: DeeperCNN_v1_best_model.pth\n",
            "Logged DeeperCNN_v1_best_model.pth as a Wandb artifact for run run-DeeperCNN_v1-20250605-141737.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>learning_rate_current</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▆▄▄▃▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.61256</td></tr><tr><td>train_loss</td><td>1.03501</td></tr><tr><td>val_accuracy</td><td>0.58373</td></tr><tr><td>val_loss</td><td>1.09594</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-DeeperCNN_v1-20250605-141737</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/cht7k3iq</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_141737-cht7k3iq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run run-DeeperCNN_v1-20250605-141737 finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class OverfitCNN(nn.Module):\n",
        "    def __init__(self, num_classes_param):\n",
        "        super(OverfitCNN, self).__init__()\n",
        "\n",
        "        # Block 1 - Wider\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # 64 filters\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # Output: (64, 24, 24)\n",
        "\n",
        "        # Block 2 - Wider\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 128 filters\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # Output: (128, 12, 12)\n",
        "\n",
        "        # Block 3 - Wider\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # 256 filters\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2, 2) # Output: (256, 6, 6)\n",
        "\n",
        "        # Block 4 - New Deep Block\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # 512 filters\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # Output: (512, 3, 3)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        # Flattened size: 512 * 3 * 3 = 512 * 9 = 4608\n",
        "        self.fc1 = nn.Linear(512 * 3 * 3, 1024) # Large FC layer\n",
        "        self.relu5 = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(1024, num_classes_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.pool3(self.relu3(self.conv3(x)))\n",
        "        x = self.pool4(self.relu4(self.conv4(x)))\n",
        "\n",
        "        x = x.view(-1, 512 * 3 * 3)\n",
        "\n",
        "        x = self.relu5(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "J3UqSRUSetVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfitting_model = OverfitCNN(num_classes_param=num_classes).to(device)\n",
        "print(\"--- OverfitCNN Architecture (Experiment 3A) ---\")\n",
        "print(overfitting_model)\n",
        "\n",
        "learning_rate_exp3a = 0.001\n",
        "optimizer_exp3a = optim.Adam(overfitting_model.parameters(), lr=learning_rate_exp3a)\n",
        "\n",
        "config_wandb_exp3a = {\n",
        "    \"learning_rate\": learning_rate_exp3a,\n",
        "    \"architecture\": \"OverfitCNN_v1_NoDropout\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\",\n",
        "    \"epochs\": 25, # Increase epochs to give more time to overfit\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.0,\n",
        "    \"conv1_filters\": 64,\n",
        "    \"conv2_filters\": 128,\n",
        "    \"conv3_filters\": 256,\n",
        "    \"conv4_filters\": 512,\n",
        "    \"fc1_units\": 1024,\n",
        "    \"notes\": \"Experiment 3A: Very deep/wide CNN with NO FC dropout, aiming to observe overfitting.\"\n",
        "}\n",
        "\n",
        "run_exp3a = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb_exp3a,\n",
        "    name=f\"run-{config_wandb_exp3a['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    reinit=True\n",
        ")\n",
        "print(f\"Wandb run for {config_wandb_exp3a['architecture']} initialized: {run_exp3a.url}\")\n",
        "\n",
        "\n",
        "if 'train_loader' in globals() and 'val_loader' in globals() and 'criterion' in globals():\n",
        "    print(f\"\\nStarting training for: {config_wandb_exp3a['architecture']}\")\n",
        "\n",
        "    best_model_file_exp3a = train_model_func(\n",
        "        model_to_train=overfitting_model,\n",
        "        t_loader=train_loader,\n",
        "        v_loader=val_loader,\n",
        "        loss_criterion=criterion,\n",
        "        opt=optimizer_exp3a,\n",
        "        num_epochs_total=config_wandb_exp3a[\"epochs\"],\n",
        "        dev=device,\n",
        "        current_wandb_run=run_exp3a,\n",
        "        run_name_prefix=config_wandb_exp3a[\"architecture\"]\n",
        "    )\n",
        "    print(f\"Best model from {config_wandb_exp3a['architecture']} training saved to: {best_model_file_exp3a}\")\n",
        "\n",
        "    trained_model_artifact_exp3a = wandb.Artifact(\n",
        "        name=f\"{config_wandb_exp3a['architecture']}_model\",\n",
        "        type=\"model\",\n",
        "        description=f\"Trained model state for {config_wandb_exp3a['architecture']}\",\n",
        "        metadata=config_wandb_exp3a\n",
        "    )\n",
        "    trained_model_artifact_exp3a.add_file(best_model_file_exp3a)\n",
        "    run_exp3a.log_artifact(trained_model_artifact_exp3a)\n",
        "    print(f\"Logged {best_model_file_exp3a} as a Wandb artifact for run {run_exp3a.name}.\")\n",
        "else:\n",
        "    print(\"Error: DataLoaders (train_loader, val_loader) or criterion not found.\")\n",
        "\n",
        "if 'run_exp3a' in globals() and run_exp3a is not None:\n",
        "    run_exp3a.finish()\n",
        "    print(f\"Wandb run {run_exp3a.name} finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "BaefnYHge8WL",
        "outputId": "29377e04-827c-4d3c-a823-e6f92bd4f0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- OverfitCNN Architecture (Experiment 3A) ---\n",
            "OverfitCNN(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu3): ReLU()\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu4): ReLU()\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=1024, bias=True)\n",
            "  (relu5): ReLU()\n",
            "  (fc2): Linear(in_features=1024, out_features=7, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_143435-tbq6yyct</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct' target=\"_blank\">run-OverfitCNN_v1_NoDropout-20250605-143435</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run for OverfitCNN_v1_NoDropout initialized: https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct\n",
            "\n",
            "Starting training for: OverfitCNN_v1_NoDropout\n",
            "Epoch [1/25], Step [100/449], Batch Loss: 1.8187\n",
            "Epoch [1/25], Step [200/449], Batch Loss: 1.5899\n",
            "Epoch [1/25], Step [300/449], Batch Loss: 1.5914\n",
            "Epoch [1/25], Step [400/449], Batch Loss: 1.6447\n",
            "Epoch [1/25]:\n",
            "  Train Loss: 1.6509, Train Acc: 0.3369\n",
            "  Val Loss: 1.4526, Val Acc: 0.4341\n",
            "New best model saved with accuracy: 0.4341 at epoch 1\n",
            "Epoch [2/25], Step [100/449], Batch Loss: 1.6167\n",
            "Epoch [2/25], Step [200/449], Batch Loss: 1.4581\n",
            "Epoch [2/25], Step [300/449], Batch Loss: 1.2598\n",
            "Epoch [2/25], Step [400/449], Batch Loss: 1.0987\n",
            "Epoch [2/25]:\n",
            "  Train Loss: 1.3685, Train Acc: 0.4740\n",
            "  Val Loss: 1.2874, Val Acc: 0.5007\n",
            "New best model saved with accuracy: 0.5007 at epoch 2\n",
            "Epoch [3/25], Step [100/449], Batch Loss: 1.2936\n",
            "Epoch [3/25], Step [200/449], Batch Loss: 1.2863\n",
            "Epoch [3/25], Step [300/449], Batch Loss: 1.3460\n",
            "Epoch [3/25], Step [400/449], Batch Loss: 1.5230\n",
            "Epoch [3/25]:\n",
            "  Train Loss: 1.2444, Train Acc: 0.5210\n",
            "  Val Loss: 1.2107, Val Acc: 0.5339\n",
            "New best model saved with accuracy: 0.5339 at epoch 3\n",
            "Epoch [4/25], Step [100/449], Batch Loss: 1.0220\n",
            "Epoch [4/25], Step [200/449], Batch Loss: 1.3490\n",
            "Epoch [4/25], Step [300/449], Batch Loss: 1.1138\n",
            "Epoch [4/25], Step [400/449], Batch Loss: 0.9337\n",
            "Epoch [4/25]:\n",
            "  Train Loss: 1.1669, Train Acc: 0.5520\n",
            "  Val Loss: 1.1741, Val Acc: 0.5581\n",
            "New best model saved with accuracy: 0.5581 at epoch 4\n",
            "Epoch [5/25], Step [100/449], Batch Loss: 1.2230\n",
            "Epoch [5/25], Step [200/449], Batch Loss: 1.0190\n",
            "Epoch [5/25], Step [300/449], Batch Loss: 1.1391\n",
            "Epoch [5/25], Step [400/449], Batch Loss: 1.0818\n",
            "Epoch [5/25]:\n",
            "  Train Loss: 1.1196, Train Acc: 0.5750\n",
            "  Val Loss: 1.1638, Val Acc: 0.5642\n",
            "New best model saved with accuracy: 0.5642 at epoch 5\n",
            "Epoch [6/25], Step [100/449], Batch Loss: 1.0651\n",
            "Epoch [6/25], Step [200/449], Batch Loss: 0.9789\n",
            "Epoch [6/25], Step [300/449], Batch Loss: 1.1707\n",
            "Epoch [6/25], Step [400/449], Batch Loss: 1.1666\n",
            "Epoch [6/25]:\n",
            "  Train Loss: 1.0783, Train Acc: 0.5868\n",
            "  Val Loss: 1.1264, Val Acc: 0.5823\n",
            "New best model saved with accuracy: 0.5823 at epoch 6\n",
            "Epoch [7/25], Step [100/449], Batch Loss: 0.9417\n",
            "Epoch [7/25], Step [200/449], Batch Loss: 1.0350\n",
            "Epoch [7/25], Step [300/449], Batch Loss: 1.0062\n",
            "Epoch [7/25], Step [400/449], Batch Loss: 0.8541\n",
            "Epoch [7/25]:\n",
            "  Train Loss: 1.0393, Train Acc: 0.6039\n",
            "  Val Loss: 1.1114, Val Acc: 0.5871\n",
            "New best model saved with accuracy: 0.5871 at epoch 7\n",
            "Epoch [8/25], Step [100/449], Batch Loss: 1.1725\n",
            "Epoch [8/25], Step [200/449], Batch Loss: 0.8148\n",
            "Epoch [8/25], Step [300/449], Batch Loss: 0.9214\n",
            "Epoch [8/25], Step [400/449], Batch Loss: 1.1628\n",
            "Epoch [8/25]:\n",
            "  Train Loss: 1.0034, Train Acc: 0.6199\n",
            "  Val Loss: 1.1502, Val Acc: 0.5848\n",
            "Epoch [9/25], Step [100/449], Batch Loss: 0.9429\n",
            "Epoch [9/25], Step [200/449], Batch Loss: 0.9480\n",
            "Epoch [9/25], Step [300/449], Batch Loss: 1.1167\n",
            "Epoch [9/25], Step [400/449], Batch Loss: 0.8595\n",
            "Epoch [9/25]:\n",
            "  Train Loss: 0.9757, Train Acc: 0.6289\n",
            "  Val Loss: 1.1606, Val Acc: 0.5837\n",
            "Epoch [10/25], Step [100/449], Batch Loss: 0.9428\n",
            "Epoch [10/25], Step [200/449], Batch Loss: 1.0078\n",
            "Epoch [10/25], Step [300/449], Batch Loss: 1.0840\n",
            "Epoch [10/25], Step [400/449], Batch Loss: 0.7692\n",
            "Epoch [10/25]:\n",
            "  Train Loss: 0.9460, Train Acc: 0.6408\n",
            "  Val Loss: 1.1091, Val Acc: 0.5918\n",
            "New best model saved with accuracy: 0.5918 at epoch 10\n",
            "Epoch [11/25], Step [100/449], Batch Loss: 0.9769\n",
            "Epoch [11/25], Step [200/449], Batch Loss: 1.1195\n",
            "Epoch [11/25], Step [300/449], Batch Loss: 0.8060\n",
            "Epoch [11/25], Step [400/449], Batch Loss: 0.9580\n",
            "Epoch [11/25]:\n",
            "  Train Loss: 0.9146, Train Acc: 0.6553\n",
            "  Val Loss: 1.1156, Val Acc: 0.5882\n",
            "Epoch [12/25], Step [100/449], Batch Loss: 1.1535\n",
            "Epoch [12/25], Step [200/449], Batch Loss: 0.7345\n",
            "Epoch [12/25], Step [300/449], Batch Loss: 1.0963\n",
            "Epoch [12/25], Step [400/449], Batch Loss: 0.9523\n",
            "Epoch [12/25]:\n",
            "  Train Loss: 0.8887, Train Acc: 0.6651\n",
            "  Val Loss: 1.1122, Val Acc: 0.5935\n",
            "New best model saved with accuracy: 0.5935 at epoch 12\n",
            "Epoch [13/25], Step [100/449], Batch Loss: 0.8746\n",
            "Epoch [13/25], Step [200/449], Batch Loss: 0.8821\n",
            "Epoch [13/25], Step [300/449], Batch Loss: 0.8750\n",
            "Epoch [13/25], Step [400/449], Batch Loss: 0.9894\n",
            "Epoch [13/25]:\n",
            "  Train Loss: 0.8564, Train Acc: 0.6771\n",
            "  Val Loss: 1.1229, Val Acc: 0.6021\n",
            "New best model saved with accuracy: 0.6021 at epoch 13\n",
            "Epoch [14/25], Step [100/449], Batch Loss: 0.9341\n",
            "Epoch [14/25], Step [200/449], Batch Loss: 0.7447\n",
            "Epoch [14/25], Step [300/449], Batch Loss: 0.7162\n",
            "Epoch [14/25], Step [400/449], Batch Loss: 0.7760\n",
            "Epoch [14/25]:\n",
            "  Train Loss: 0.8282, Train Acc: 0.6877\n",
            "  Val Loss: 1.1399, Val Acc: 0.6030\n",
            "New best model saved with accuracy: 0.6030 at epoch 14\n",
            "Epoch [15/25], Step [100/449], Batch Loss: 0.6776\n",
            "Epoch [15/25], Step [200/449], Batch Loss: 0.7451\n",
            "Epoch [15/25], Step [300/449], Batch Loss: 0.6925\n",
            "Epoch [15/25], Step [400/449], Batch Loss: 0.8708\n",
            "Epoch [15/25]:\n",
            "  Train Loss: 0.7957, Train Acc: 0.7004\n",
            "  Val Loss: 1.1496, Val Acc: 0.5952\n",
            "Epoch [16/25], Step [100/449], Batch Loss: 0.8023\n",
            "Epoch [16/25], Step [200/449], Batch Loss: 0.8957\n",
            "Epoch [16/25], Step [300/449], Batch Loss: 0.9528\n",
            "Epoch [16/25], Step [400/449], Batch Loss: 1.0038\n",
            "Epoch [16/25]:\n",
            "  Train Loss: 0.7653, Train Acc: 0.7118\n",
            "  Val Loss: 1.2032, Val Acc: 0.6060\n",
            "New best model saved with accuracy: 0.6060 at epoch 16\n",
            "Epoch [17/25], Step [100/449], Batch Loss: 0.7958\n",
            "Epoch [17/25], Step [200/449], Batch Loss: 0.7083\n",
            "Epoch [17/25], Step [300/449], Batch Loss: 0.7626\n",
            "Epoch [17/25], Step [400/449], Batch Loss: 0.8002\n",
            "Epoch [17/25]:\n",
            "  Train Loss: 0.7349, Train Acc: 0.7237\n",
            "  Val Loss: 1.1706, Val Acc: 0.6046\n",
            "Epoch [18/25], Step [100/449], Batch Loss: 0.6087\n",
            "Epoch [18/25], Step [200/449], Batch Loss: 0.6532\n",
            "Epoch [18/25], Step [300/449], Batch Loss: 0.5550\n",
            "Epoch [18/25], Step [400/449], Batch Loss: 1.1233\n",
            "Epoch [18/25]:\n",
            "  Train Loss: 0.7113, Train Acc: 0.7343\n",
            "  Val Loss: 1.2047, Val Acc: 0.6027\n",
            "Epoch [19/25], Step [100/449], Batch Loss: 0.6474\n",
            "Epoch [19/25], Step [200/449], Batch Loss: 0.7425\n",
            "Epoch [19/25], Step [300/449], Batch Loss: 0.7355\n",
            "Epoch [19/25], Step [400/449], Batch Loss: 0.7069\n",
            "Epoch [19/25]:\n",
            "  Train Loss: 0.6834, Train Acc: 0.7464\n",
            "  Val Loss: 1.2205, Val Acc: 0.6010\n",
            "Epoch [20/25], Step [100/449], Batch Loss: 0.6640\n",
            "Epoch [20/25], Step [200/449], Batch Loss: 0.9335\n",
            "Epoch [20/25], Step [300/449], Batch Loss: 0.6724\n",
            "Epoch [20/25], Step [400/449], Batch Loss: 0.6822\n",
            "Epoch [20/25]:\n",
            "  Train Loss: 0.6534, Train Acc: 0.7545\n",
            "  Val Loss: 1.2569, Val Acc: 0.6038\n",
            "Epoch [21/25], Step [100/449], Batch Loss: 0.6616\n",
            "Epoch [21/25], Step [200/449], Batch Loss: 0.6190\n",
            "Epoch [21/25], Step [300/449], Batch Loss: 0.6234\n",
            "Epoch [21/25], Step [400/449], Batch Loss: 0.7355\n",
            "Epoch [21/25]:\n",
            "  Train Loss: 0.6258, Train Acc: 0.7699\n",
            "  Val Loss: 1.2911, Val Acc: 0.6032\n",
            "Epoch [22/25], Step [100/449], Batch Loss: 0.7620\n",
            "Epoch [22/25], Step [200/449], Batch Loss: 0.4876\n",
            "Epoch [22/25], Step [300/449], Batch Loss: 0.6549\n",
            "Epoch [22/25], Step [400/449], Batch Loss: 0.6980\n",
            "Epoch [22/25]:\n",
            "  Train Loss: 0.5978, Train Acc: 0.7795\n",
            "  Val Loss: 1.4070, Val Acc: 0.5952\n",
            "Epoch [23/25], Step [100/449], Batch Loss: 0.5774\n",
            "Epoch [23/25], Step [200/449], Batch Loss: 0.5935\n",
            "Epoch [23/25], Step [300/449], Batch Loss: 0.5858\n",
            "Epoch [23/25], Step [400/449], Batch Loss: 0.4895\n",
            "Epoch [23/25]:\n",
            "  Train Loss: 0.5874, Train Acc: 0.7818\n",
            "  Val Loss: 1.3909, Val Acc: 0.5999\n",
            "Epoch [24/25], Step [100/449], Batch Loss: 0.7476\n",
            "Epoch [24/25], Step [200/449], Batch Loss: 0.5169\n",
            "Epoch [24/25], Step [300/449], Batch Loss: 0.4401\n",
            "Epoch [24/25], Step [400/449], Batch Loss: 0.5253\n",
            "Epoch [24/25]:\n",
            "  Train Loss: 0.5605, Train Acc: 0.7939\n",
            "  Val Loss: 1.4263, Val Acc: 0.5988\n",
            "Epoch [25/25], Step [100/449], Batch Loss: 0.4847\n",
            "Epoch [25/25], Step [200/449], Batch Loss: 0.6712\n",
            "Epoch [25/25], Step [300/449], Batch Loss: 0.6727\n",
            "Epoch [25/25], Step [400/449], Batch Loss: 0.5754\n",
            "Epoch [25/25]:\n",
            "  Train Loss: 0.5446, Train Acc: 0.8005\n",
            "  Val Loss: 1.3891, Val Acc: 0.5943\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.6060\n",
            "Best model from OverfitCNN_v1_NoDropout training saved to: OverfitCNN_v1_NoDropout_best_model.pth\n",
            "Logged OverfitCNN_v1_NoDropout_best_model.pth as a Wandb artifact for run run-OverfitCNN_v1_NoDropout-20250605-143435.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>learning_rate_current</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▂▂▁▁▁▁▂▂▃▂▃▃▄▅▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.80052</td></tr><tr><td>train_loss</td><td>0.54465</td></tr><tr><td>val_accuracy</td><td>0.59432</td></tr><tr><td>val_loss</td><td>1.38908</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-OverfitCNN_v1_NoDropout-20250605-143435</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/tbq6yyct</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_143435-tbq6yyct/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run run-OverfitCNN_v1_NoDropout-20250605-143435 finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedOverfitCNN(nn.Module):\n",
        "    def __init__(self, num_classes_param):\n",
        "        super(RegularizedOverfitCNN, self).__init__()\n",
        "        # --- Block 1 ---\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64) # ADDED BN\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # self.drop2d1 = nn.Dropout2d(p=0.25) # Optional: Spatial Dropout\n",
        "\n",
        "        # --- Block 2 ---\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128) # ADDED BN\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # --- Block 3 ---\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256) # ADDED BN\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        # self.drop2d2 = nn.Dropout2d(p=0.25) # Optional\n",
        "\n",
        "        # --- Block 4 ---\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512) # ADDED BN\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(512 * 3 * 3, 1024)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(1024) # ADDED BN for FC\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(p=0.5) # ADDED/INCREASED FC Dropout\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, num_classes_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        # x = self.drop2d1(x)\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
        "        # x = self.drop2d2(x)\n",
        "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 3 * 3)\n",
        "        x = self.dropout_fc(self.relu5(self.bn_fc1(self.fc1(x))))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "v9QkYZoke5jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_exp3b = RegularizedOverfitCNN(num_classes_param=num_classes).to(device)\n",
        "print(model_exp3b)\n",
        "\n",
        "learning_rate_exp3b = 0.001\n",
        "optimizer_reg_overfit = optim.Adam(model_exp3b.parameters(), lr=learning_rate_exp3b)\n",
        "\n",
        "config_wandb_exp3b = {\n",
        "    \"learning_rate\": learning_rate_exp3b,\n",
        "    \"architecture\": \"RegularizedOverfitCNN_v1\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\",\n",
        "    \"epochs\": 25, # Same as OverfitCNN for comparison\n",
        "    \"batch_size\": batch_size,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.5, # Added/Increased dropout\n",
        "    \"batch_norm_conv\": True, # Indicated Batch Norm is used\n",
        "    \"batch_norm_fc\": True,\n",
        "    \"weight_decay\": 1e-4, # If you added it\n",
        "    # ... (other params like filter counts if they are the same as OverfitCNN)\n",
        "    \"notes\": \"Experiment 3B: Regularized version of OverfitCNN with BN, Dropout 0.5, and L2.\"\n",
        "}\n",
        "\n",
        "# For Experiment 3B\n",
        "run_exp3b = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb_exp3b,\n",
        "    name=f\"run-{config_wandb_exp3b['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    reinit=True\n",
        ")\n",
        "\n",
        "# 5. Train the new model (reusing train_model_func)\n",
        "if 'train_loader' in globals() and 'val_loader' in globals() and 'criterion' in globals():\n",
        "    print(f\"\\nStarting training for: {config_wandb_exp3b['architecture']}\")\n",
        "\n",
        "    best_model_file_exp3b = train_model_func(\n",
        "        model_to_train=model_exp3b, # Pass the correct model\n",
        "        t_loader=train_loader,\n",
        "        v_loader=val_loader,\n",
        "        loss_criterion=criterion,\n",
        "        opt=optimizer_reg_overfit, # Pass the correct optimizer\n",
        "        num_epochs_total=config_wandb_exp3b[\"epochs\"],\n",
        "        dev=device,\n",
        "        current_wandb_run=run_exp3b, # Pass the correct Wandb run object\n",
        "        run_name_prefix=config_wandb_exp3b[\"architecture\"]\n",
        "    )\n",
        "    print(f\"Best model from {config_wandb_exp3b['architecture']} training saved to: {best_model_file_exp3b}\")\n",
        "\n",
        "    # Log the trained model as an artifact\n",
        "    trained_model_artifact_exp3b = wandb.Artifact(\n",
        "        name=f\"{config_wandb_exp3b['architecture']}_model\",\n",
        "        type=\"model\",\n",
        "        description=f\"Trained model state for {config_wandb_exp3a['architecture']}\",\n",
        "        metadata=config_wandb_exp3b\n",
        "    )\n",
        "    trained_model_artifact_exp3b.add_file(best_model_file_exp3b)\n",
        "    run_exp3b.log_artifact(trained_model_artifact_exp3b)\n",
        "    print(f\"Logged {best_model_file_exp3a} as a Wandb artifact for run {run_exp3b.name}.\")\n",
        "else:\n",
        "    print(\"Error: DataLoaders (train_loader, val_loader) or criterion not found.\")\n",
        "\n",
        "# 6. Finish the Wandb run for Experiment 3A\n",
        "if 'run_exp3b' in globals() and run_exp3a is not None:\n",
        "    run_exp3b.finish()\n",
        "    print(f\"Wandb run {run_exp3b.name} finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LBXwtUiWhGZR",
        "outputId": "78e2e508-4a4a-48a3-b054-def4e762dc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegularizedOverfitCNN(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu3): ReLU()\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu4): ReLU()\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=1024, bias=True)\n",
            "  (bn_fc1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu5): ReLU()\n",
            "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=1024, out_features=7, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_145737-4db36ht3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/4db36ht3' target=\"_blank\">run-RegularizedOverfitCNN_v1-20250605-145737</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/4db36ht3' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/4db36ht3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training for: RegularizedOverfitCNN_v1\n",
            "Epoch [1/25], Step [100/449], Batch Loss: 1.5278\n",
            "Epoch [1/25], Step [200/449], Batch Loss: 1.4911\n",
            "Epoch [1/25], Step [300/449], Batch Loss: 1.4309\n",
            "Epoch [1/25], Step [400/449], Batch Loss: 1.5793\n",
            "Epoch [1/25]:\n",
            "  Train Loss: 1.5148, Train Acc: 0.4197\n",
            "  Val Loss: 1.3042, Val Acc: 0.5085\n",
            "New best model saved with accuracy: 0.5085 at epoch 1\n",
            "Epoch [2/25], Step [100/449], Batch Loss: 1.3449\n",
            "Epoch [2/25], Step [200/449], Batch Loss: 1.1668\n",
            "Epoch [2/25], Step [300/449], Batch Loss: 1.3037\n",
            "Epoch [2/25], Step [400/449], Batch Loss: 1.3896\n",
            "Epoch [2/25]:\n",
            "  Train Loss: 1.2497, Train Acc: 0.5246\n",
            "  Val Loss: 1.2413, Val Acc: 0.5308\n",
            "New best model saved with accuracy: 0.5308 at epoch 2\n",
            "Epoch [3/25], Step [100/449], Batch Loss: 1.3317\n",
            "Epoch [3/25], Step [200/449], Batch Loss: 1.4426\n",
            "Epoch [3/25], Step [300/449], Batch Loss: 1.0556\n",
            "Epoch [3/25], Step [400/449], Batch Loss: 0.9671\n",
            "Epoch [3/25]:\n",
            "  Train Loss: 1.1626, Train Acc: 0.5599\n",
            "  Val Loss: 1.1618, Val Acc: 0.5508\n",
            "New best model saved with accuracy: 0.5508 at epoch 3\n",
            "Epoch [4/25], Step [100/449], Batch Loss: 1.2256\n",
            "Epoch [4/25], Step [200/449], Batch Loss: 1.0266\n",
            "Epoch [4/25], Step [300/449], Batch Loss: 1.0818\n",
            "Epoch [4/25], Step [400/449], Batch Loss: 1.0686\n",
            "Epoch [4/25]:\n",
            "  Train Loss: 1.1018, Train Acc: 0.5817\n",
            "  Val Loss: 1.1700, Val Acc: 0.5678\n",
            "New best model saved with accuracy: 0.5678 at epoch 4\n",
            "Epoch [5/25], Step [100/449], Batch Loss: 1.2389\n",
            "Epoch [5/25], Step [200/449], Batch Loss: 1.1974\n",
            "Epoch [5/25], Step [300/449], Batch Loss: 1.0318\n",
            "Epoch [5/25], Step [400/449], Batch Loss: 0.9263\n",
            "Epoch [5/25]:\n",
            "  Train Loss: 1.0578, Train Acc: 0.6023\n",
            "  Val Loss: 1.1130, Val Acc: 0.5921\n",
            "New best model saved with accuracy: 0.5921 at epoch 5\n",
            "Epoch [6/25], Step [100/449], Batch Loss: 0.8670\n",
            "Epoch [6/25], Step [200/449], Batch Loss: 0.8985\n",
            "Epoch [6/25], Step [300/449], Batch Loss: 0.9840\n",
            "Epoch [6/25], Step [400/449], Batch Loss: 1.1165\n",
            "Epoch [6/25]:\n",
            "  Train Loss: 1.0161, Train Acc: 0.6184\n",
            "  Val Loss: 1.0853, Val Acc: 0.5971\n",
            "New best model saved with accuracy: 0.5971 at epoch 6\n",
            "Epoch [7/25], Step [100/449], Batch Loss: 0.8205\n",
            "Epoch [7/25], Step [200/449], Batch Loss: 1.1335\n",
            "Epoch [7/25], Step [300/449], Batch Loss: 0.6922\n",
            "Epoch [7/25], Step [400/449], Batch Loss: 1.1375\n",
            "Epoch [7/25]:\n",
            "  Train Loss: 0.9735, Train Acc: 0.6326\n",
            "  Val Loss: 1.0430, Val Acc: 0.6133\n",
            "New best model saved with accuracy: 0.6133 at epoch 7\n",
            "Epoch [8/25], Step [100/449], Batch Loss: 1.0122\n",
            "Epoch [8/25], Step [200/449], Batch Loss: 0.8381\n",
            "Epoch [8/25], Step [300/449], Batch Loss: 0.9934\n",
            "Epoch [8/25], Step [400/449], Batch Loss: 0.8581\n",
            "Epoch [8/25]:\n",
            "  Train Loss: 0.9346, Train Acc: 0.6484\n",
            "  Val Loss: 1.0789, Val Acc: 0.6091\n",
            "Epoch [9/25], Step [100/449], Batch Loss: 0.8490\n",
            "Epoch [9/25], Step [200/449], Batch Loss: 1.1004\n",
            "Epoch [9/25], Step [300/449], Batch Loss: 0.8658\n",
            "Epoch [9/25], Step [400/449], Batch Loss: 0.6830\n",
            "Epoch [9/25]:\n",
            "  Train Loss: 0.8944, Train Acc: 0.6626\n",
            "  Val Loss: 1.0909, Val Acc: 0.6177\n",
            "New best model saved with accuracy: 0.6177 at epoch 9\n",
            "Epoch [10/25], Step [100/449], Batch Loss: 0.8629\n",
            "Epoch [10/25], Step [200/449], Batch Loss: 0.8385\n",
            "Epoch [10/25], Step [300/449], Batch Loss: 0.8948\n",
            "Epoch [10/25], Step [400/449], Batch Loss: 0.8916\n",
            "Epoch [10/25]:\n",
            "  Train Loss: 0.8575, Train Acc: 0.6790\n",
            "  Val Loss: 1.0660, Val Acc: 0.6172\n",
            "Epoch [11/25], Step [100/449], Batch Loss: 0.8601\n",
            "Epoch [11/25], Step [200/449], Batch Loss: 0.7905\n",
            "Epoch [11/25], Step [300/449], Batch Loss: 0.8847\n",
            "Epoch [11/25], Step [400/449], Batch Loss: 1.1935\n",
            "Epoch [11/25]:\n",
            "  Train Loss: 0.8103, Train Acc: 0.6942\n",
            "  Val Loss: 1.0601, Val Acc: 0.6183\n",
            "New best model saved with accuracy: 0.6183 at epoch 11\n",
            "Epoch [12/25], Step [100/449], Batch Loss: 0.6214\n",
            "Epoch [12/25], Step [200/449], Batch Loss: 0.7845\n",
            "Epoch [12/25], Step [300/449], Batch Loss: 0.9075\n",
            "Epoch [12/25], Step [400/449], Batch Loss: 1.0263\n",
            "Epoch [12/25]:\n",
            "  Train Loss: 0.7816, Train Acc: 0.7119\n",
            "  Val Loss: 1.0870, Val Acc: 0.6311\n",
            "New best model saved with accuracy: 0.6311 at epoch 12\n",
            "Epoch [13/25], Step [100/449], Batch Loss: 0.6048\n",
            "Epoch [13/25], Step [200/449], Batch Loss: 0.5919\n",
            "Epoch [13/25], Step [300/449], Batch Loss: 0.8737\n",
            "Epoch [13/25], Step [400/449], Batch Loss: 0.7241\n",
            "Epoch [13/25]:\n",
            "  Train Loss: 0.7419, Train Acc: 0.7219\n",
            "  Val Loss: 1.0731, Val Acc: 0.6406\n",
            "New best model saved with accuracy: 0.6406 at epoch 13\n",
            "Epoch [14/25], Step [100/449], Batch Loss: 0.5305\n",
            "Epoch [14/25], Step [200/449], Batch Loss: 0.7337\n",
            "Epoch [14/25], Step [300/449], Batch Loss: 0.8744\n",
            "Epoch [14/25], Step [400/449], Batch Loss: 0.6323\n",
            "Epoch [14/25]:\n",
            "  Train Loss: 0.6983, Train Acc: 0.7419\n",
            "  Val Loss: 1.0589, Val Acc: 0.6414\n",
            "New best model saved with accuracy: 0.6414 at epoch 14\n",
            "Epoch [15/25], Step [100/449], Batch Loss: 0.5413\n",
            "Epoch [15/25], Step [200/449], Batch Loss: 0.5656\n",
            "Epoch [15/25], Step [300/449], Batch Loss: 0.7072\n",
            "Epoch [15/25], Step [400/449], Batch Loss: 0.7733\n",
            "Epoch [15/25]:\n",
            "  Train Loss: 0.6587, Train Acc: 0.7564\n",
            "  Val Loss: 1.1209, Val Acc: 0.6328\n",
            "Epoch [16/25], Step [100/449], Batch Loss: 0.3872\n",
            "Epoch [16/25], Step [200/449], Batch Loss: 0.7914\n",
            "Epoch [16/25], Step [300/449], Batch Loss: 0.6247\n",
            "Epoch [16/25], Step [400/449], Batch Loss: 0.5348\n",
            "Epoch [16/25]:\n",
            "  Train Loss: 0.6234, Train Acc: 0.7731\n",
            "  Val Loss: 1.1544, Val Acc: 0.6397\n",
            "Epoch [17/25], Step [100/449], Batch Loss: 0.7472\n",
            "Epoch [17/25], Step [200/449], Batch Loss: 0.4833\n",
            "Epoch [17/25], Step [300/449], Batch Loss: 0.6784\n",
            "Epoch [17/25], Step [400/449], Batch Loss: 0.7669\n",
            "Epoch [17/25]:\n",
            "  Train Loss: 0.5879, Train Acc: 0.7841\n",
            "  Val Loss: 1.1312, Val Acc: 0.6512\n",
            "New best model saved with accuracy: 0.6512 at epoch 17\n",
            "Epoch [18/25], Step [100/449], Batch Loss: 0.3325\n",
            "Epoch [18/25], Step [200/449], Batch Loss: 0.4447\n",
            "Epoch [18/25], Step [300/449], Batch Loss: 0.4296\n",
            "Epoch [18/25], Step [400/449], Batch Loss: 0.6481\n",
            "Epoch [18/25]:\n",
            "  Train Loss: 0.5538, Train Acc: 0.7954\n",
            "  Val Loss: 1.1622, Val Acc: 0.6422\n",
            "Epoch [19/25], Step [100/449], Batch Loss: 0.3363\n",
            "Epoch [19/25], Step [200/449], Batch Loss: 0.4950\n",
            "Epoch [19/25], Step [300/449], Batch Loss: 0.5249\n",
            "Epoch [19/25], Step [400/449], Batch Loss: 0.3675\n",
            "Epoch [19/25]:\n",
            "  Train Loss: 0.5163, Train Acc: 0.8138\n",
            "  Val Loss: 1.2143, Val Acc: 0.6358\n",
            "Epoch [20/25], Step [100/449], Batch Loss: 0.4239\n",
            "Epoch [20/25], Step [200/449], Batch Loss: 0.4577\n",
            "Epoch [20/25], Step [300/449], Batch Loss: 0.5398\n",
            "Epoch [20/25], Step [400/449], Batch Loss: 0.5118\n",
            "Epoch [20/25]:\n",
            "  Train Loss: 0.4855, Train Acc: 0.8232\n",
            "  Val Loss: 1.2454, Val Acc: 0.6403\n",
            "Epoch [21/25], Step [100/449], Batch Loss: 0.3586\n",
            "Epoch [21/25], Step [200/449], Batch Loss: 0.3852\n",
            "Epoch [21/25], Step [300/449], Batch Loss: 0.3863\n",
            "Epoch [21/25], Step [400/449], Batch Loss: 0.3573\n",
            "Epoch [21/25]:\n",
            "  Train Loss: 0.4599, Train Acc: 0.8336\n",
            "  Val Loss: 1.2540, Val Acc: 0.6542\n",
            "New best model saved with accuracy: 0.6542 at epoch 21\n",
            "Epoch [22/25], Step [100/449], Batch Loss: 0.4610\n",
            "Epoch [22/25], Step [200/449], Batch Loss: 0.3669\n",
            "Epoch [22/25], Step [300/449], Batch Loss: 0.2870\n",
            "Epoch [22/25], Step [400/449], Batch Loss: 0.6706\n",
            "Epoch [22/25]:\n",
            "  Train Loss: 0.4358, Train Acc: 0.8430\n",
            "  Val Loss: 1.3300, Val Acc: 0.6422\n",
            "Epoch [23/25], Step [100/449], Batch Loss: 0.4790\n",
            "Epoch [23/25], Step [200/449], Batch Loss: 0.2863\n",
            "Epoch [23/25], Step [300/449], Batch Loss: 0.3358\n",
            "Epoch [23/25], Step [400/449], Batch Loss: 0.5084\n",
            "Epoch [23/25]:\n",
            "  Train Loss: 0.4192, Train Acc: 0.8496\n",
            "  Val Loss: 1.3474, Val Acc: 0.6445\n",
            "Epoch [24/25], Step [100/449], Batch Loss: 0.2578\n",
            "Epoch [24/25], Step [200/449], Batch Loss: 0.3410\n",
            "Epoch [24/25], Step [300/449], Batch Loss: 0.4975\n",
            "Epoch [24/25], Step [400/449], Batch Loss: 0.3108\n",
            "Epoch [24/25]:\n",
            "  Train Loss: 0.3874, Train Acc: 0.8612\n",
            "  Val Loss: 1.3140, Val Acc: 0.6389\n",
            "Epoch [25/25], Step [100/449], Batch Loss: 0.4295\n",
            "Epoch [25/25], Step [200/449], Batch Loss: 0.3581\n",
            "Epoch [25/25], Step [300/449], Batch Loss: 0.3287\n",
            "Epoch [25/25], Step [400/449], Batch Loss: 0.5570\n",
            "Epoch [25/25]:\n",
            "  Train Loss: 0.3701, Train Acc: 0.8653\n",
            "  Val Loss: 1.3708, Val Acc: 0.6514\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.6542\n",
            "Best model from RegularizedOverfitCNN_v1 training saved to: RegularizedOverfitCNN_v1_best_model.pth\n",
            "Logged OverfitCNN_v1_NoDropout_best_model.pth as a Wandb artifact for run run-RegularizedOverfitCNN_v1-20250605-145737.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>learning_rate_current</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇█▇▇▇█▇█▇█</td></tr><tr><td>val_loss</td><td>▇▅▄▄▂▂▁▂▂▁▁▂▂▁▃▃▃▄▅▅▆▇█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.86527</td></tr><tr><td>train_loss</td><td>0.37008</td></tr><tr><td>val_accuracy</td><td>0.65143</td></tr><tr><td>val_loss</td><td>1.37082</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-RegularizedOverfitCNN_v1-20250605-145737</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/4db36ht3' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/4db36ht3</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_145737-4db36ht3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run run-RegularizedOverfitCNN_v1-20250605-145737 finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Setting up Experiment 4: DeeperCNN_v1 for More Epochs ---\")\n",
        "\n",
        "model_exp4 = DeeperCNN(num_classes_param=num_classes).to(device)\n",
        "print(\"--- DeeperCNN Architecture (Experiment 4) ---\")\n",
        "print(model_exp4)\n",
        "\n",
        "# Using similar settings as before for fair comparison of epoch effect\n",
        "learning_rate_exp4 = 0.001\n",
        "optimizer_exp4 = optim.Adam(model_exp4.parameters(), lr=learning_rate_exp4)\n",
        "\n",
        "config_wandb_exp4 = {\n",
        "    \"learning_rate\": learning_rate_exp4,\n",
        "    \"architecture\": \"DeeperCNN_v1_MoreEpochs\",\n",
        "    \"dataset\": \"FER2013_from_icml_face_data\",\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": batch_size, # Global batch_size\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"criterion\": \"CrossEntropyLoss\",\n",
        "    \"dropout_fc\": 0.5,\n",
        "    \"conv1_filters\": 32,\n",
        "    \"conv2_filters\": 64,\n",
        "    \"conv3_filters\": 128,\n",
        "    \"fc1_units\": 256,\n",
        "    \"notes\": \"Experiment 4: Training DeeperCNN_v1 for 30 epochs to check for further improvement or overfitting.\"\n",
        "}\n",
        "\n",
        "run_exp4 = wandb.init(\n",
        "    project=\"facial-expression-recognition-challenge\",\n",
        "    config=config_wandb_exp4,\n",
        "    name=f\"run-{config_wandb_exp4['architecture']}-{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    reinit=True\n",
        ")\n",
        "print(f\"Wandb run for {config_wandb_exp4['architecture']} initialized: {run_exp4.url if run_exp4 else 'Failed to init'}\")\n",
        "\n",
        "if 'train_loader' in globals() and 'val_loader' in globals() and 'criterion' in globals() and run_exp4:\n",
        "    print(f\"\\nStarting training for: {config_wandb_exp4['architecture']}\")\n",
        "\n",
        "    best_model_file_exp4 = train_model_func(\n",
        "        model_to_train=model_exp4,\n",
        "        t_loader=train_loader,\n",
        "        v_loader=val_loader,\n",
        "        loss_criterion=criterion,\n",
        "        opt=optimizer_exp4,\n",
        "        num_epochs_total=config_wandb_exp4[\"epochs\"],\n",
        "        dev=device,\n",
        "        current_wandb_run=run_exp4,\n",
        "        run_name_prefix=config_wandb_exp4[\"architecture\"]\n",
        "    )\n",
        "    print(f\"Best model from {config_wandb_exp4['architecture']} training saved to: {best_model_file_exp4}\")\n",
        "\n",
        "    if best_model_file_exp4 and os.path.exists(best_model_file_exp4):\n",
        "        trained_model_artifact_exp4 = wandb.Artifact(\n",
        "            name=f\"{config_wandb_exp4['architecture']}_model\",\n",
        "            type=\"model\",\n",
        "            description=f\"Trained model state for {config_wandb_exp4['architecture']}\",\n",
        "            metadata=config_wandb_exp4\n",
        "        )\n",
        "        trained_model_artifact_exp4.add_file(best_model_file_exp4)\n",
        "        run_exp4.log_artifact(trained_model_artifact_exp4)\n",
        "        print(f\"Logged {best_model_file_exp4} as a Wandb artifact for run {run_exp4.name}.\")\n",
        "    else:\n",
        "        print(f\"Model file {best_model_file_exp4} not found for artifact logging.\")\n",
        "else:\n",
        "    print(\"Error: DataLoaders, criterion, or Wandb run not properly initialized for Experiment 4.\")\n",
        "\n",
        "if 'run_exp4' in globals() and run_exp4 is not None:\n",
        "    run_exp4.finish()\n",
        "    print(f\"Wandb run {run_exp4.name} finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EABvlI4OnKAN",
        "outputId": "082273ee-791a-4c8e-f704-dfe4ab617584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up Experiment 4: DeeperCNN_v1 for More Epochs ---\n",
            "--- DeeperCNN Architecture (Experiment 4) ---\n",
            "DeeperCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu3): ReLU()\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4608, out_features=256, bias=True)\n",
            "  (relu4): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_150511-9yqwhg11</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11' target=\"_blank\">run-DeeperCNN_v1_MoreEpochs-20250605-150511</a></strong> to <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run for DeeperCNN_v1_MoreEpochs initialized: https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11\n",
            "\n",
            "Starting training for: DeeperCNN_v1_MoreEpochs\n",
            "Epoch [1/30], Step [100/449], Batch Loss: 1.7497\n",
            "Epoch [1/30], Step [200/449], Batch Loss: 1.7105\n",
            "Epoch [1/30], Step [300/449], Batch Loss: 1.7407\n",
            "Epoch [1/30], Step [400/449], Batch Loss: 1.6035\n",
            "Epoch [1/30]:\n",
            "  Train Loss: 1.7152, Train Acc: 0.3085\n",
            "  Val Loss: 1.5598, Val Acc: 0.4082\n",
            "New best model saved with accuracy: 0.4082 at epoch 1\n",
            "Epoch [2/30], Step [100/449], Batch Loss: 1.4795\n",
            "Epoch [2/30], Step [200/449], Batch Loss: 1.5225\n",
            "Epoch [2/30], Step [300/449], Batch Loss: 1.3752\n",
            "Epoch [2/30], Step [400/449], Batch Loss: 1.3754\n",
            "Epoch [2/30]:\n",
            "  Train Loss: 1.5242, Train Acc: 0.4096\n",
            "  Val Loss: 1.4109, Val Acc: 0.4522\n",
            "New best model saved with accuracy: 0.4522 at epoch 2\n",
            "Epoch [3/30], Step [100/449], Batch Loss: 1.3455\n",
            "Epoch [3/30], Step [200/449], Batch Loss: 1.2322\n",
            "Epoch [3/30], Step [300/449], Batch Loss: 1.1731\n",
            "Epoch [3/30], Step [400/449], Batch Loss: 1.4770\n",
            "Epoch [3/30]:\n",
            "  Train Loss: 1.4109, Train Acc: 0.4579\n",
            "  Val Loss: 1.3307, Val Acc: 0.4854\n",
            "New best model saved with accuracy: 0.4854 at epoch 3\n",
            "Epoch [4/30], Step [100/449], Batch Loss: 1.2813\n",
            "Epoch [4/30], Step [200/449], Batch Loss: 1.5702\n",
            "Epoch [4/30], Step [300/449], Batch Loss: 1.3703\n",
            "Epoch [4/30], Step [400/449], Batch Loss: 1.1353\n",
            "Epoch [4/30]:\n",
            "  Train Loss: 1.3436, Train Acc: 0.4851\n",
            "  Val Loss: 1.3026, Val Acc: 0.4987\n",
            "New best model saved with accuracy: 0.4987 at epoch 4\n",
            "Epoch [5/30], Step [100/449], Batch Loss: 1.3057\n",
            "Epoch [5/30], Step [200/449], Batch Loss: 1.3857\n",
            "Epoch [5/30], Step [300/449], Batch Loss: 1.1782\n",
            "Epoch [5/30], Step [400/449], Batch Loss: 1.1665\n",
            "Epoch [5/30]:\n",
            "  Train Loss: 1.3014, Train Acc: 0.5057\n",
            "  Val Loss: 1.2348, Val Acc: 0.5291\n",
            "New best model saved with accuracy: 0.5291 at epoch 5\n",
            "Epoch [6/30], Step [100/449], Batch Loss: 1.2891\n",
            "Epoch [6/30], Step [200/449], Batch Loss: 0.9545\n",
            "Epoch [6/30], Step [300/449], Batch Loss: 1.2124\n",
            "Epoch [6/30], Step [400/449], Batch Loss: 1.3915\n",
            "Epoch [6/30]:\n",
            "  Train Loss: 1.2650, Train Acc: 0.5181\n",
            "  Val Loss: 1.2300, Val Acc: 0.5227\n",
            "Epoch [7/30], Step [100/449], Batch Loss: 1.2530\n",
            "Epoch [7/30], Step [200/449], Batch Loss: 1.2434\n",
            "Epoch [7/30], Step [300/449], Batch Loss: 1.1408\n",
            "Epoch [7/30], Step [400/449], Batch Loss: 1.1102\n",
            "Epoch [7/30]:\n",
            "  Train Loss: 1.2400, Train Acc: 0.5311\n",
            "  Val Loss: 1.1896, Val Acc: 0.5327\n",
            "New best model saved with accuracy: 0.5327 at epoch 7\n",
            "Epoch [8/30], Step [100/449], Batch Loss: 1.5689\n",
            "Epoch [8/30], Step [200/449], Batch Loss: 1.2191\n",
            "Epoch [8/30], Step [300/449], Batch Loss: 1.4112\n",
            "Epoch [8/30], Step [400/449], Batch Loss: 1.0450\n",
            "Epoch [8/30]:\n",
            "  Train Loss: 1.2141, Train Acc: 0.5414\n",
            "  Val Loss: 1.1826, Val Acc: 0.5414\n",
            "New best model saved with accuracy: 0.5414 at epoch 8\n",
            "Epoch [9/30], Step [100/449], Batch Loss: 1.1415\n",
            "Epoch [9/30], Step [200/449], Batch Loss: 1.2768\n",
            "Epoch [9/30], Step [300/449], Batch Loss: 1.1333\n",
            "Epoch [9/30], Step [400/449], Batch Loss: 1.1032\n",
            "Epoch [9/30]:\n",
            "  Train Loss: 1.1956, Train Acc: 0.5438\n",
            "  Val Loss: 1.1620, Val Acc: 0.5511\n",
            "New best model saved with accuracy: 0.5511 at epoch 9\n",
            "Epoch [10/30], Step [100/449], Batch Loss: 1.2586\n",
            "Epoch [10/30], Step [200/449], Batch Loss: 0.9484\n",
            "Epoch [10/30], Step [300/449], Batch Loss: 1.1978\n",
            "Epoch [10/30], Step [400/449], Batch Loss: 1.1460\n",
            "Epoch [10/30]:\n",
            "  Train Loss: 1.1782, Train Acc: 0.5528\n",
            "  Val Loss: 1.1522, Val Acc: 0.5614\n",
            "New best model saved with accuracy: 0.5614 at epoch 10\n",
            "Epoch [11/30], Step [100/449], Batch Loss: 1.2376\n",
            "Epoch [11/30], Step [200/449], Batch Loss: 1.0902\n",
            "Epoch [11/30], Step [300/449], Batch Loss: 1.1525\n",
            "Epoch [11/30], Step [400/449], Batch Loss: 0.9776\n",
            "Epoch [11/30]:\n",
            "  Train Loss: 1.1671, Train Acc: 0.5574\n",
            "  Val Loss: 1.1529, Val Acc: 0.5511\n",
            "Epoch [12/30], Step [100/449], Batch Loss: 1.0295\n",
            "Epoch [12/30], Step [200/449], Batch Loss: 1.2085\n",
            "Epoch [12/30], Step [300/449], Batch Loss: 1.1086\n",
            "Epoch [12/30], Step [400/449], Batch Loss: 1.1625\n",
            "Epoch [12/30]:\n",
            "  Train Loss: 1.1418, Train Acc: 0.5639\n",
            "  Val Loss: 1.1423, Val Acc: 0.5592\n",
            "Epoch [13/30], Step [100/449], Batch Loss: 1.2671\n",
            "Epoch [13/30], Step [200/449], Batch Loss: 1.1019\n",
            "Epoch [13/30], Step [300/449], Batch Loss: 0.9944\n",
            "Epoch [13/30], Step [400/449], Batch Loss: 1.1771\n",
            "Epoch [13/30]:\n",
            "  Train Loss: 1.1312, Train Acc: 0.5715\n",
            "  Val Loss: 1.1316, Val Acc: 0.5598\n",
            "Epoch [14/30], Step [100/449], Batch Loss: 1.1675\n",
            "Epoch [14/30], Step [200/449], Batch Loss: 1.1931\n",
            "Epoch [14/30], Step [300/449], Batch Loss: 1.2454\n",
            "Epoch [14/30], Step [400/449], Batch Loss: 1.1889\n",
            "Epoch [14/30]:\n",
            "  Train Loss: 1.1203, Train Acc: 0.5762\n",
            "  Val Loss: 1.1177, Val Acc: 0.5731\n",
            "New best model saved with accuracy: 0.5731 at epoch 14\n",
            "Epoch [15/30], Step [100/449], Batch Loss: 1.1356\n",
            "Epoch [15/30], Step [200/449], Batch Loss: 0.9793\n",
            "Epoch [15/30], Step [300/449], Batch Loss: 1.1899\n",
            "Epoch [15/30], Step [400/449], Batch Loss: 1.1718\n",
            "Epoch [15/30]:\n",
            "  Train Loss: 1.1036, Train Acc: 0.5822\n",
            "  Val Loss: 1.1287, Val Acc: 0.5659\n",
            "Epoch [16/30], Step [100/449], Batch Loss: 1.0330\n",
            "Epoch [16/30], Step [200/449], Batch Loss: 1.0360\n",
            "Epoch [16/30], Step [300/449], Batch Loss: 1.2138\n",
            "Epoch [16/30], Step [400/449], Batch Loss: 1.1481\n",
            "Epoch [16/30]:\n",
            "  Train Loss: 1.1004, Train Acc: 0.5842\n",
            "  Val Loss: 1.1125, Val Acc: 0.5762\n",
            "New best model saved with accuracy: 0.5762 at epoch 16\n",
            "Epoch [17/30], Step [100/449], Batch Loss: 1.0425\n",
            "Epoch [17/30], Step [200/449], Batch Loss: 1.2343\n",
            "Epoch [17/30], Step [300/449], Batch Loss: 1.0973\n",
            "Epoch [17/30], Step [400/449], Batch Loss: 1.2072\n",
            "Epoch [17/30]:\n",
            "  Train Loss: 1.0892, Train Acc: 0.5873\n",
            "  Val Loss: 1.1077, Val Acc: 0.5782\n",
            "New best model saved with accuracy: 0.5782 at epoch 17\n",
            "Epoch [18/30], Step [100/449], Batch Loss: 1.0197\n",
            "Epoch [18/30], Step [200/449], Batch Loss: 1.1762\n",
            "Epoch [18/30], Step [300/449], Batch Loss: 0.9841\n",
            "Epoch [18/30], Step [400/449], Batch Loss: 1.1276\n",
            "Epoch [18/30]:\n",
            "  Train Loss: 1.0774, Train Acc: 0.5896\n",
            "  Val Loss: 1.1023, Val Acc: 0.5770\n",
            "Epoch [19/30], Step [100/449], Batch Loss: 1.1628\n",
            "Epoch [19/30], Step [200/449], Batch Loss: 1.2578\n",
            "Epoch [19/30], Step [300/449], Batch Loss: 0.9272\n",
            "Epoch [19/30], Step [400/449], Batch Loss: 0.8107\n",
            "Epoch [19/30]:\n",
            "  Train Loss: 1.0695, Train Acc: 0.5919\n",
            "  Val Loss: 1.1192, Val Acc: 0.5809\n",
            "New best model saved with accuracy: 0.5809 at epoch 19\n",
            "Epoch [20/30], Step [100/449], Batch Loss: 0.9883\n",
            "Epoch [20/30], Step [200/449], Batch Loss: 1.1476\n",
            "Epoch [20/30], Step [300/449], Batch Loss: 1.0634\n",
            "Epoch [20/30], Step [400/449], Batch Loss: 1.0182\n",
            "Epoch [20/30]:\n",
            "  Train Loss: 1.0610, Train Acc: 0.5969\n",
            "  Val Loss: 1.1013, Val Acc: 0.5776\n",
            "Epoch [21/30], Step [100/449], Batch Loss: 1.0490\n",
            "Epoch [21/30], Step [200/449], Batch Loss: 1.0332\n",
            "Epoch [21/30], Step [300/449], Batch Loss: 1.0311\n",
            "Epoch [21/30], Step [400/449], Batch Loss: 1.1963\n",
            "Epoch [21/30]:\n",
            "  Train Loss: 1.0535, Train Acc: 0.6002\n",
            "  Val Loss: 1.1056, Val Acc: 0.5821\n",
            "New best model saved with accuracy: 0.5821 at epoch 21\n",
            "Epoch [22/30], Step [100/449], Batch Loss: 1.0201\n",
            "Epoch [22/30], Step [200/449], Batch Loss: 1.1749\n",
            "Epoch [22/30], Step [300/449], Batch Loss: 1.0053\n",
            "Epoch [22/30], Step [400/449], Batch Loss: 0.9873\n",
            "Epoch [22/30]:\n",
            "  Train Loss: 1.0463, Train Acc: 0.6049\n",
            "  Val Loss: 1.1099, Val Acc: 0.5795\n",
            "Epoch [23/30], Step [100/449], Batch Loss: 1.0901\n",
            "Epoch [23/30], Step [200/449], Batch Loss: 0.9943\n",
            "Epoch [23/30], Step [300/449], Batch Loss: 1.0980\n",
            "Epoch [23/30], Step [400/449], Batch Loss: 1.1812\n",
            "Epoch [23/30]:\n",
            "  Train Loss: 1.0378, Train Acc: 0.6068\n",
            "  Val Loss: 1.1290, Val Acc: 0.5762\n",
            "Epoch [24/30], Step [100/449], Batch Loss: 0.8965\n",
            "Epoch [24/30], Step [200/449], Batch Loss: 1.2145\n",
            "Epoch [24/30], Step [300/449], Batch Loss: 1.0489\n",
            "Epoch [24/30], Step [400/449], Batch Loss: 1.0221\n",
            "Epoch [24/30]:\n",
            "  Train Loss: 1.0318, Train Acc: 0.6091\n",
            "  Val Loss: 1.0975, Val Acc: 0.5860\n",
            "New best model saved with accuracy: 0.5860 at epoch 24\n",
            "Epoch [25/30], Step [100/449], Batch Loss: 0.8854\n",
            "Epoch [25/30], Step [200/449], Batch Loss: 0.9730\n",
            "Epoch [25/30], Step [300/449], Batch Loss: 1.2072\n",
            "Epoch [25/30], Step [400/449], Batch Loss: 1.2365\n",
            "Epoch [25/30]:\n",
            "  Train Loss: 1.0195, Train Acc: 0.6120\n",
            "  Val Loss: 1.0985, Val Acc: 0.5854\n",
            "Epoch [26/30], Step [100/449], Batch Loss: 0.8695\n",
            "Epoch [26/30], Step [200/449], Batch Loss: 1.0183\n",
            "Epoch [26/30], Step [300/449], Batch Loss: 1.0137\n",
            "Epoch [26/30], Step [400/449], Batch Loss: 0.9460\n",
            "Epoch [26/30]:\n",
            "  Train Loss: 1.0165, Train Acc: 0.6128\n",
            "  Val Loss: 1.1097, Val Acc: 0.5871\n",
            "New best model saved with accuracy: 0.5871 at epoch 26\n",
            "Epoch [27/30], Step [100/449], Batch Loss: 1.0248\n",
            "Epoch [27/30], Step [200/449], Batch Loss: 1.0071\n",
            "Epoch [27/30], Step [300/449], Batch Loss: 1.0222\n",
            "Epoch [27/30], Step [400/449], Batch Loss: 0.9353\n",
            "Epoch [27/30]:\n",
            "  Train Loss: 1.0054, Train Acc: 0.6202\n",
            "  Val Loss: 1.1013, Val Acc: 0.5812\n",
            "Epoch [28/30], Step [100/449], Batch Loss: 1.0360\n",
            "Epoch [28/30], Step [200/449], Batch Loss: 1.0983\n",
            "Epoch [28/30], Step [300/449], Batch Loss: 1.0576\n",
            "Epoch [28/30], Step [400/449], Batch Loss: 1.1951\n",
            "Epoch [28/30]:\n",
            "  Train Loss: 1.0012, Train Acc: 0.6179\n",
            "  Val Loss: 1.0944, Val Acc: 0.5885\n",
            "New best model saved with accuracy: 0.5885 at epoch 28\n",
            "Epoch [29/30], Step [100/449], Batch Loss: 1.0070\n",
            "Epoch [29/30], Step [200/449], Batch Loss: 1.0681\n",
            "Epoch [29/30], Step [300/449], Batch Loss: 0.7511\n",
            "Epoch [29/30], Step [400/449], Batch Loss: 0.8644\n",
            "Epoch [29/30]:\n",
            "  Train Loss: 0.9957, Train Acc: 0.6243\n",
            "  Val Loss: 1.1202, Val Acc: 0.5893\n",
            "New best model saved with accuracy: 0.5893 at epoch 29\n",
            "Epoch [30/30], Step [100/449], Batch Loss: 0.7896\n",
            "Epoch [30/30], Step [200/449], Batch Loss: 0.8150\n",
            "Epoch [30/30], Step [300/449], Batch Loss: 0.9714\n",
            "Epoch [30/30], Step [400/449], Batch Loss: 1.0102\n",
            "Epoch [30/30]:\n",
            "  Train Loss: 0.9920, Train Acc: 0.6249\n",
            "  Val Loss: 1.1030, Val Acc: 0.6030\n",
            "New best model saved with accuracy: 0.6030 at epoch 30\n",
            "Finished Training\n",
            "Best validation accuracy during training: 0.6030\n",
            "Best model from DeeperCNN_v1_MoreEpochs training saved to: DeeperCNN_v1_MoreEpochs_best_model.pth\n",
            "Logged DeeperCNN_v1_MoreEpochs_best_model.pth as a Wandb artifact for run run-DeeperCNN_v1_MoreEpochs-20250605-150511.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>learning_rate_current</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▅▅▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate_current</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.62486</td></tr><tr><td>train_loss</td><td>0.99202</td></tr><tr><td>val_accuracy</td><td>0.60295</td></tr><tr><td>val_loss</td><td>1.103</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run-DeeperCNN_v1_MoreEpochs-20250605-150511</strong> at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge/runs/9yqwhg11</a><br> View project at: <a href='https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge' target=\"_blank\">https://wandb.ai/egabe21-free-university-of-tbilisi-/facial-expression-recognition-challenge</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_150511-9yqwhg11/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb run run-DeeperCNN_v1_MoreEpochs-20250605-150511 finished.\n"
          ]
        }
      ]
    }
  ]
}